{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9350e9f0-5844-4f50-9834-7a669c9006f2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Problem Statement:**\n",
    "\n",
    "Goal: Create a unified hybrid framework that adapts to the commodity type and automatically selects or combines the best models accordingly.\n",
    "\n",
    "We loaded historical data from 2003-April 2025 alonmg with new realtime data to predict price with pre trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc731a-53b5-4620-b088-efb51fbc8dd7",
   "metadata": {},
   "source": [
    "## Step 8 Processing Real Time Data with Pre Trained Models\n",
    "\n",
    "##### Sourcing Key Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2fe22d6-25d1-4d64-8b49-1379718636b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import os \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "# from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "#from pmdarima.arima import auto_arima\n",
    "import pmdarima as pm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import os\n",
    "import itertools\n",
    "import requests\n",
    "import yfinance as yf\n",
    "from scipy.stats import randint\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "#from arch import arch_model\n",
    "from transformers import pipeline\n",
    "#from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b2c08-188a-4f03-9f68-74982a216b65",
   "metadata": {},
   "source": [
    "##### Define constant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa3528a-b459-48a0-9c2d-a515f0723723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constant values\n",
    "# Your API Key from FRED\n",
    "api_key = 'a27b910873da479a5561ea08035a6c79'\n",
    "# API Key for Alpha Vantage (Replace with your actual key)\n",
    "API_KEY_alpha = \"3J4EGZCB0D7UT9WG\"\n",
    "# genertae your APi key from here: https://www.alphavantage.co/\n",
    "\n",
    "#update the below code for any new precious Metal\n",
    "\n",
    "data_ticker_map = {\n",
    "            'GOLD' : 'GC=F',\n",
    "            'SILVER': 'SI=F',\n",
    "            'CRUDE_OIL': 'CL=F'}\n",
    "\n",
    "data_file_mapping = {\n",
    "            'GOLD' : '../../Data/Historical_Gold_data_April_2003.csv',\n",
    "           'SILVER': '../../Data/Historical_Silver_data_April_2003.csv',\n",
    "           'CRUDE_OIL': '../../Data/Historical_Crude_oil_data_April_2003.csv',}\n",
    "\n",
    "Live_Sentiments_file_name = 'Live_Sentiments.csv'\n",
    "Live_Sentiments_file_dir = '../../Sentiments'\n",
    "Historical_Sentiments_file_name = 'Historical_Sentiments.csv'\n",
    "Historical_Sentiments_file_dir = '../../Sentiments'\n",
    "\n",
    "best_models_dir = {'GOLD': '../../Models/GOLD_MODELS', 'SILVER' : '../../Models/SILVER_MODELS', 'CRUDE_OIL': '../../Models/CRUDE_OIL_MODELS'}\n",
    "hybrid_models_dir = {'GOLD': '../../Models/GOLD_HYBRID_MODELS', 'SILVER' : '../../Models/SILVER_HYBRID_MODELS', 'CRUDE_OIL': '../../Models/CRUDE_OIL_HYBRID_MODELS'}\n",
    "hybrid_meta_models_dir = {'GOLD': '../../Models/GOLD_HYBRID_MODELS/Meta', 'SILVER' : '../../Models/SILVER_HYBRID_MODELS/Meta', 'CRUDE_OIL': '../../Models/CRUDE_OIL_HYBRID_MODELS/Meta'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db13ce-91b8-4dad-859f-4036235e59b7",
   "metadata": {},
   "source": [
    "### Enter Commodity Name for which you want to predict Price EX: GOLD, SILVER, CRUDE OIL etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705fe956-2422-454f-87be-2ac355994ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity = 'CRUDE_OIL' #input('Enter the Commodity Name')\n",
    "filename =  data_file_mapping.get(commodity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ca900-0131-4051-95ee-8391f7f091b2",
   "metadata": {},
   "source": [
    "## Data Sourcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ee017-b4f3-4f1a-aba3-79a7a7754262",
   "metadata": {},
   "source": [
    "### Historical Data Sourcing\n",
    "\n",
    "##### Yahoo finance is not reliable and having issues, download data from 2003-01-01 - 2025-03-15 via yf api."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f93fae-040f-4126-a2c1-9865a4fcff41",
   "metadata": {},
   "source": [
    "##### Define function to download Historical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5993c70-5986-4f42-bb63-7c6315de6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download historical price data\n",
    "def download_price_data(ticker, start='2003-01-01', end='2025-04-28'):\n",
    "    # Download the data\n",
    "    data = yf.download(ticker, start=start, end=end)\n",
    "    \n",
    "    # Reset index to turn 'Date' into a regular column\n",
    "    data.reset_index(inplace=True)\n",
    "    \n",
    "    # Flatten the columns if there are multiple levels\n",
    "    data.columns = data.columns.get_level_values(0) if isinstance(data.columns, pd.MultiIndex) else data.columns\n",
    "    \n",
    "    # Select necessary columns only\n",
    "    data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "    return data\n",
    "\n",
    "# Load or download data for each commodity\n",
    "def load_or_download_data(ticker, filename):\n",
    "    if os.path.exists(filename):\n",
    "        # Read the CSV, treating the first row as the header\n",
    "        data = pd.read_csv(filename, header=0)\n",
    "        \n",
    "        # Convert 'Date' column to datetime format, and handle parsing errors\n",
    "        data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n",
    "        \n",
    "        # Remove any rows where 'Date' could not be parsed and resulted in NaT\n",
    "        data = data.dropna(subset=['Date'])\n",
    "    else:\n",
    "        # Download the data\n",
    "        data = download_price_data(ticker)\n",
    "        \n",
    "        # Save to CSV with a single header row\n",
    "        data.to_csv(filename, index=False)\n",
    "    \n",
    "    # Ensure the 'Close' column is numeric\n",
    "    data['Close'] = pd.to_numeric(data['Close'], errors='coerce')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2a3e34-735b-4477-9271-bc0add04b847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from Historical file id Data available in file : ../../Data/Historical_Crude_oil_data_April_2003.csv\n",
      "Ticker code for CRUDE_OIL : CL=F\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "############# Reading Data Sources #################\n",
    "print(f'Loading from Historical file id Data available in file : {filename}')\n",
    "ticker = data_ticker_map.get(commodity)\n",
    "print(f'Ticker code for {commodity} : {ticker}')\n",
    "commodity_data = load_or_download_data(ticker, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7f3311-e940-4c9d-a3d3-ec7c9f71a68e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Price</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>62480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>68416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>32.650002</td>\n",
       "      <td>32.849998</td>\n",
       "      <td>31.910000</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>98247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-07</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>31.080000</td>\n",
       "      <td>124279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-08</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>108037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>64.300003</td>\n",
       "      <td>64.419998</td>\n",
       "      <td>62.450001</td>\n",
       "      <td>63.080002</td>\n",
       "      <td>82671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>2025-04-22</td>\n",
       "      <td>63.430000</td>\n",
       "      <td>65.089996</td>\n",
       "      <td>63.430000</td>\n",
       "      <td>64.309998</td>\n",
       "      <td>297928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.870003</td>\n",
       "      <td>61.529999</td>\n",
       "      <td>62.270000</td>\n",
       "      <td>397841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>62.340000</td>\n",
       "      <td>63.310001</td>\n",
       "      <td>61.990002</td>\n",
       "      <td>62.790001</td>\n",
       "      <td>264908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5608 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price       Date       Open       High        Low      Close  Volume\n",
       "0     2003-01-02  31.549999  32.090000  31.400000  31.850000   62480\n",
       "1     2003-01-03  32.080002  33.200001  31.900000  33.080002   68416\n",
       "2     2003-01-06  32.650002  32.849998  31.910000  32.099998   98247\n",
       "3     2003-01-07  31.549999  31.700001  30.510000  31.080000  124279\n",
       "4     2003-01-08  30.250000  30.700001  29.750000  30.559999  108037\n",
       "...          ...        ...        ...        ...        ...     ...\n",
       "5603  2025-04-21  64.300003  64.419998  62.450001  63.080002   82671\n",
       "5604  2025-04-22  63.430000  65.089996  63.430000  64.309998  297928\n",
       "5605  2025-04-23  64.000000  64.870003  61.529999  62.270000  397841\n",
       "5606  2025-04-24  62.340000  63.310001  61.990002  62.790001  264908\n",
       "5607  2025-04-25  62.860001  63.410000  61.799999  63.020000  283758\n",
       "\n",
       "[5608 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0b1de-a054-4fe8-b221-eb83e5b00995",
   "metadata": {},
   "source": [
    "### Sourcing Market data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92b23c1-8321-4785-b8e3-4e9fb8a86b81",
   "metadata": {},
   "source": [
    "####  GDP data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f663d-4beb-4fa9-8d87-781b4f50d8a8",
   "metadata": {},
   "source": [
    "##### Define function which will convert Yearly GDP data to daily GDP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f27a97c8-2cb5-44a5-b4f5-de2132bb9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_gdp_to_daily(gdp_df):\n",
    "    daily_gdp = []\n",
    "    for index, row in gdp_df.iterrows():\n",
    "        month_start = row['Date']\n",
    "        value = row['Value']\n",
    "        month_days = pd.date_range(start=month_start, end=month_start + pd.offsets.MonthEnd(0), freq='D')\n",
    "        for day in month_days:\n",
    "            daily_gdp.append({'Date': day, 'Value': value})\n",
    "    return pd.DataFrame(daily_gdp)\n",
    "\n",
    "\n",
    "def preprocess_gdp_data(gdp_data):\n",
    "    gdp_data['Date'] = pd.to_datetime(gdp_data['Date'])\n",
    "    return expand_gdp_to_daily(gdp_data)\n",
    "\n",
    "\n",
    "def fetch_world_bank_data(indicator, country='USA', start_year='2003', end_year='2025'):\n",
    "    url = f'http://api.worldbank.org/v2/country/{country}/indicator/{indicator}?date={start_year}:{end_year}&format=json'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if len(data) < 2 or 'message' in data[1]:\n",
    "            print(\"No data found for the specified parameters.\")\n",
    "            return None\n",
    "        df = pd.json_normalize(data[1])\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df[(df['date'] >= f'{start_year}-01-01') & (df['date'] <= f'{end_year}-12-31')]\n",
    "        df = df[['date', 'value']]\n",
    "        df.columns = ['Date', 'Value']\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30dbe92-f02b-499f-8c08-d2802a038022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-02</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-03</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>2003-01-27</td>\n",
       "      <td>11456450000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>2003-01-28</td>\n",
       "      <td>11456450000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>2003-01-29</td>\n",
       "      <td>11456450000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>2003-01-30</td>\n",
       "      <td>11456450000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>2003-01-31</td>\n",
       "      <td>11456450000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>682 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date               GDP\n",
       "0   2024-01-01               NaT\n",
       "1   2024-01-02               NaT\n",
       "2   2024-01-03               NaT\n",
       "3   2024-01-04               NaT\n",
       "4   2024-01-05               NaT\n",
       "..         ...               ...\n",
       "677 2003-01-27  11456450000000.0\n",
       "678 2003-01-28  11456450000000.0\n",
       "679 2003-01-29  11456450000000.0\n",
       "680 2003-01-30  11456450000000.0\n",
       "681 2003-01-31  11456450000000.0\n",
       "\n",
       "[682 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### sourcing world GDP data #################################################\n",
    "gdp_data = fetch_world_bank_data('NY.GDP.MKTP.CD', country='USA', start_year='2003', end_year='2025')\n",
    "daily_gdp_data = preprocess_gdp_data(gdp_data)\n",
    "daily_gdp_data.rename(columns={'Value':'GDP'},inplace=True)\n",
    "daily_gdp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c76cd28-d934-415c-b38f-e9f82c663ecc",
   "metadata": {},
   "source": [
    "####  Interest rate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93c134a1-53b0-4f5e-aea3-cc1ee200a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\1493415932.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_interest_rate_data.rename(columns={'date':'Date'},inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Interest_rates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1954-07-01</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1954-07-02</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1954-07-03</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1954-07-04</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954-07-05</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25838</th>\n",
       "      <td>2025-03-28</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25839</th>\n",
       "      <td>2025-03-29</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25840</th>\n",
       "      <td>2025-03-30</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25841</th>\n",
       "      <td>2025-03-31</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25842</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25843 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Interest_rates\n",
       "0     1954-07-01            0.80\n",
       "1     1954-07-02            0.80\n",
       "2     1954-07-03            0.80\n",
       "3     1954-07-04            0.80\n",
       "4     1954-07-05            0.80\n",
       "...          ...             ...\n",
       "25838 2025-03-28            4.33\n",
       "25839 2025-03-29            4.33\n",
       "25840 2025-03-30            4.33\n",
       "25841 2025-03-31            4.33\n",
       "25842 2025-04-01            4.33\n",
       "\n",
       "[25843 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "############## Sourcing Interest rate data ################################\n",
    "from fredapi import Fred\n",
    "\n",
    "# Your API Key from FRED\n",
    "api_key = 'a27b910873da479a5561ea08035a6c79'\n",
    "\n",
    "# Initialize the FRED API\n",
    "fred = Fred(api_key=api_key)\n",
    "\n",
    "# Fetch the Federal Funds Rate data (you can change this to any other indicator)\n",
    "# Federal Funds Rate is often identified by the FRED ID 'FEDFUNDS'\n",
    "interest_rate_data = fred.get_series('FEDFUNDS' ,start_date='2003-01-01')\n",
    "\n",
    "# Convert the data into a DataFrame for better handling\n",
    "interest_rate_df = pd.DataFrame(interest_rate_data)\n",
    "interest_rate_df.columns = ['Interest_rates']\n",
    "\n",
    "# Reset the index to have 'Date' as a column\n",
    "interest_rate_df.reset_index(inplace=True)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `interest_rate_df` already exists with 'Date' and 'Interest_rates' columns\n",
    "\n",
    "# Convert 'Date' column to datetime format if it’s not already\n",
    "interest_rate_df['Date'] = pd.to_datetime(interest_rate_df['index'])\n",
    "interest_rate_df.drop(columns=['index'], inplace=True)  # Drop the old index column if needed\n",
    "\n",
    "# Create a date range for every day from the start to the end of the interest rate data\n",
    "start_date = interest_rate_df['Date'].min()\n",
    "end_date = interest_rate_df['Date'].max()\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Create a new DataFrame with daily dates as a column\n",
    "daily_interest_rate_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Add a 'month' and 'year' column to `interest_rate_df` for easy merging\n",
    "interest_rate_df['year'] = interest_rate_df['Date'].dt.year\n",
    "interest_rate_df['month'] = interest_rate_df['Date'].dt.month\n",
    "\n",
    "# Merge monthly data with daily data on matching 'year' and 'month'\n",
    "daily_interest_rate_df['year'] = daily_interest_rate_df['date'].dt.year\n",
    "daily_interest_rate_df['month'] = daily_interest_rate_df['date'].dt.month\n",
    "\n",
    "# Perform a left join on 'year' and 'month' columns\n",
    "daily_interest_rate_df = daily_interest_rate_df.merge(\n",
    "    interest_rate_df[['year', 'month', 'Interest_rates']],\n",
    "    on=['year', 'month'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the extra columns, keep only 'date' and 'Interest_rates'\n",
    "daily_interest_rate_data = daily_interest_rate_df[['date', 'Interest_rates']]\n",
    "daily_interest_rate_data.rename(columns={'date':'Date'},inplace=True)\n",
    "daily_interest_rate_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ee9a5-1801-49d5-81ea-c456500d1d16",
   "metadata": {},
   "source": [
    "####  GDP to Debt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "567f72fe-d4e9-4468-b58e-bbd7b0f7cb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DebtToGDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>33.267642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>33.267642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>33.267642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>33.267642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>33.267642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>2022-12-28</td>\n",
       "      <td>110.385332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>110.385332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>110.385332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>110.385332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8402 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   DebtToGDP\n",
       "0    2000-01-01   33.267642\n",
       "1    2000-01-02   33.267642\n",
       "2    2000-01-03   33.267642\n",
       "3    2000-01-04   33.267642\n",
       "4    2000-01-05   33.267642\n",
       "...         ...         ...\n",
       "8397 2022-12-28  110.385332\n",
       "8398 2022-12-29  110.385332\n",
       "8399 2022-12-30  110.385332\n",
       "8400 2022-12-31  110.385332\n",
       "8401 2023-01-01    0.000000\n",
       "\n",
       "[8402 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## GDP to Debt DATA ###############################\n",
    "import wbdata\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Set the start and end dates for the data\n",
    "start_date = datetime.datetime(2000, 1, 1)\n",
    "end_date = datetime.datetime(2023, 1, 1)\n",
    "\n",
    "# List of indicators to retrieve (Government Debt to GDP ratio)\n",
    "indicators = {'GC.DOD.TOTL.GD.ZS': 'Government Debt to GDP'}\n",
    "\n",
    "# Fetch data from the World Bank API for the USA\n",
    "government_debt_data = wbdata.get_dataframe(indicators, country='USA')\n",
    "\n",
    "# Fill missing values in `Government Debt to GDP` with 0\n",
    "government_debt_data['Government Debt to GDP'] = government_debt_data['Government Debt to GDP'].fillna(0)\n",
    "\n",
    "# Reset index to make `date` a regular column and convert it to datetime\n",
    "government_debt_data.reset_index(inplace=True)\n",
    "government_debt_data['date'] = pd.to_datetime(government_debt_data['date'])\n",
    "\n",
    "# Add a 'year' column to `government_debt_data` to help with mapping\n",
    "government_debt_data['year'] = government_debt_data['date'].dt.year\n",
    "\n",
    "# Create a dictionary for quick lookup of Debt to GDP per year\n",
    "debt_to_gdp_dict = government_debt_data.set_index('year')['Government Debt to GDP'].to_dict()\n",
    "\n",
    "# Create a date range for every day from the start to the end date\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Create a new DataFrame with daily dates as a column\n",
    "daily_debt_data = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Map each date in `daily_debt_data` to the corresponding Debt to GDP value for that year\n",
    "daily_debt_data['DebtToGDP'] = daily_debt_data['date'].dt.year.map(debt_to_gdp_dict)\n",
    "daily_debt_data.rename(columns={'date':'Date'},inplace=True)\n",
    "\n",
    "daily_debt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e825e2-39b4-4c68-a964-455dba12a9cd",
   "metadata": {},
   "source": [
    "####  Inflation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1ecd240-27db-4ed3-8a77-28265c97b00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Inflation_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1954-07-01</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1954-08-01</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1954-09-01</td>\n",
       "      <td>1.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1954-10-01</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954-11-01</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>2024-12-01</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>2025-02-01</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>849</th>\n",
       "      <td>2025-04-01</td>\n",
       "      <td>4.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>850 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Inflation_rate\n",
       "0   1954-07-01            0.80\n",
       "1   1954-08-01            1.22\n",
       "2   1954-09-01            1.07\n",
       "3   1954-10-01            0.85\n",
       "4   1954-11-01            0.83\n",
       "..         ...             ...\n",
       "845 2024-12-01            4.48\n",
       "846 2025-01-01            4.33\n",
       "847 2025-02-01            4.33\n",
       "848 2025-03-01            4.33\n",
       "849 2025-04-01            4.33\n",
       "\n",
       "[850 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############## inflation data #################\n",
    "# Get historical CPI data (Consumer Price Index) to calculate inflation\n",
    "cpi_data = fred.get_series('CPIAUCSL', start_date='2003-01-01')\n",
    "\n",
    "\n",
    "\n",
    "# Convert the data into a DataFrame for better handling\n",
    "Inflation_data = pd.DataFrame(interest_rate_data)\n",
    "Inflation_data.columns = ['Inflation_rate']\n",
    "\n",
    "# Reset the index to have 'Date' as a column\n",
    "Inflation_data.reset_index(inplace=True)\n",
    "Inflation_data.rename(columns={'index':'Date'},inplace=True)\n",
    "# Print the first few rows\n",
    "Inflation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fba95-b00b-45b6-aad1-0f3b6e657f3b",
   "metadata": {},
   "source": [
    "####  ETF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119bd0ae-8466-40c9-87d4-4d7426fafca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full JSON response: {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'observation_start': '2003-01-01', 'observation_end': '2024-11-01', 'units': 'lin', 'output_type': 1, 'file_type': 'json', 'order_by': 'observation_date', 'sort_order': 'asc', 'count': 224, 'offset': 0, 'limit': 100000, 'observations': [{'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-01-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-02-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-03-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-04-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-05-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-06-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-07-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-08-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-09-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-10-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-11-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2003-12-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-01-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-02-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-03-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-04-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-05-01', 'value': '2'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-06-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-07-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-08-01', 'value': '2.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-09-01', 'value': '2.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-10-01', 'value': '2.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-11-01', 'value': '3'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2004-12-01', 'value': '3.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-01-01', 'value': '3.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-02-01', 'value': '3.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-03-01', 'value': '3.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-04-01', 'value': '3.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-05-01', 'value': '4'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-06-01', 'value': '4.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-07-01', 'value': '4.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-08-01', 'value': '4.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-09-01', 'value': '4.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-10-01', 'value': '4.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-11-01', 'value': '5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2005-12-01', 'value': '5.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-01-01', 'value': '5.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-02-01', 'value': '5.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-03-01', 'value': '5.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-04-01', 'value': '5.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-05-01', 'value': '6'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-06-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-07-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-08-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-09-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-10-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-11-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2006-12-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-01-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-02-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-03-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-04-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-05-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-06-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-07-01', 'value': '6.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-08-01', 'value': '5.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-09-01', 'value': '5.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-10-01', 'value': '5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-11-01', 'value': '5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2007-12-01', 'value': '4.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-01-01', 'value': '3.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-02-01', 'value': '3.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-03-01', 'value': '2.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-04-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-05-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-06-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-07-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-08-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-09-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-10-01', 'value': '1.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-11-01', 'value': '1.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2008-12-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-01-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-02-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-03-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-04-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-05-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-06-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-07-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-08-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-09-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-10-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-11-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2009-12-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-01-01', 'value': '0.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-02-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-03-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-04-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-05-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-06-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-07-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-08-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-09-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-10-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-11-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2010-12-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-01-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-02-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-03-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-04-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-05-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-06-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-07-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-08-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-09-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-10-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-11-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2011-12-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-01-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-02-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-03-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-04-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-05-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-06-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-07-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-08-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-09-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-10-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-11-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2012-12-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-01-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-02-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-03-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-04-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-05-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-06-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-07-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-08-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-09-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-10-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-11-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2013-12-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-01-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-02-01', 'value': '0.75000'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-03-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-04-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-05-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-06-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-07-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-08-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-09-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-10-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-11-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2014-12-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-01-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-02-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-03-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-04-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-05-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-06-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-07-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-08-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-09-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-10-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-11-01', 'value': '0.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2015-12-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-01-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-02-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-03-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-04-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-05-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-06-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-07-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-08-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-09-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-10-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-11-01', 'value': '1'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2016-12-01', 'value': '1.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-01-01', 'value': '1.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-02-01', 'value': '1.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-03-01', 'value': '1.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-04-01', 'value': '1.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-05-01', 'value': '1.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-06-01', 'value': '1.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-07-01', 'value': '1.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-08-01', 'value': '1.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-09-01', 'value': '1.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-10-01', 'value': '1.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-11-01', 'value': '1.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2017-12-01', 'value': '2.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-01-01', 'value': '2.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-02-01', 'value': '2.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-03-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-04-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-05-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-06-01', 'value': '2.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-07-01', 'value': '2.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-08-01', 'value': '2.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-09-01', 'value': '2.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-10-01', 'value': '2.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-11-01', 'value': '2.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2018-12-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-01-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-02-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-03-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-04-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-05-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-06-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-07-01', 'value': '3.0'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-08-01', 'value': '2.75'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-09-01', 'value': '2.5'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-10-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-11-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2019-12-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-01-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-02-01', 'value': '2.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-03-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-04-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-05-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-06-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-07-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-08-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-09-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-10-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-11-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2020-12-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-01-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-02-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-03-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-04-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-05-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-06-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-07-01', 'value': '0.25'}, {'realtime_start': '2025-04-12', 'realtime_end': '2025-04-12', 'date': '2021-08-01', 'value': '0.25'}]}\n",
      "           realtime_start realtime_end        date  ETF_Value\n",
      "Date                                                         \n",
      "2003-01-01     2025-04-12   2025-04-12  2003-01-01       2.25\n",
      "2003-02-01     2025-04-12   2025-04-12  2003-02-01       2.25\n",
      "2003-03-01     2025-04-12   2025-04-12  2003-03-01       2.25\n",
      "2003-04-01     2025-04-12   2025-04-12  2003-04-01       2.25\n",
      "2003-05-01     2025-04-12   2025-04-12  2003-05-01       2.25\n"
     ]
    }
   ],
   "source": [
    "################ Gold ETF data ###########################################\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Fetch ETF data from FRED API\n",
    "def fetch_fred_etf_data(series_id, api_key, start_date='2003-01-01', end_date='2024-11-01'):\n",
    "    url = f'https://api.stlouisfed.org/fred/series/observations?series_id={series_id}&api_key={api_key}&file_type=json&observation_start={start_date}&observation_end={end_date}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Print the entire JSON response for inspection\n",
    "    print(\"Full JSON response:\", data)\n",
    "    \n",
    "    # Check for errors in the JSON response\n",
    "    if \"observations\" not in data:\n",
    "        print(\"Error: 'observations' key not found in the response. Check the series ID or API request.\")\n",
    "        return None\n",
    "    \n",
    "    # Parse JSON data into a DataFrame\n",
    "    etf_df = pd.DataFrame(data['observations'])\n",
    "    etf_df['Date'] = pd.to_datetime(etf_df['date'])\n",
    "    etf_df['Date'] = etf_df['Date'].dt.tz_localize(None).dt.date \n",
    "    etf_df['value'] = pd.to_numeric(etf_df['value'], errors='coerce')\n",
    "    etf_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    return etf_df\n",
    "\n",
    "# Example usage\n",
    "api_key = 'a27b910873da479a5561ea08035a6c79'\n",
    "series_id = 'INTDSRUSM193N'  # Replace with a valid series ID from FRED\n",
    "etf_data = fetch_fred_etf_data(series_id, api_key)\n",
    "\n",
    "\n",
    "etf_data.rename(columns={'value':'ETF_Value'},inplace=True)\n",
    "# Only print if data retrieval was successful\n",
    "if etf_data is not None:\n",
    "    print(etf_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b06659b-180b-48dd-9d21-4ed1c8d47cf5",
   "metadata": {},
   "source": [
    "## Sourcing Sentiment Data and Apply BERT\n",
    "\n",
    "#### Sourcing Live Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa600903-d3cb-4193-8b66-d00908731511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up NewsAPI\n",
    "API_KEY = \"250e051931514de6b0e5120412c4e3ad\"\n",
    "NEWS_API_URL = \"https://newsapi.org/v2/everything\"\n",
    "\n",
    "# Fetch news using query\n",
    "def fetch_news(query, language=\"en\", page_size=100):\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"language\": language,\n",
    "        \"apiKey\": API_KEY,\n",
    "        \"pageSize\": page_size,\n",
    "        \"sortBy\": \"publishedAt\",\n",
    "    }\n",
    "    response = requests.get(NEWS_API_URL, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data.get(\"status\") == \"ok\":\n",
    "        articles = data.get(\"articles\", [])\n",
    "        return [(article[\"publishedAt\"], article[\"title\"]) for article in articles]\n",
    "    else:\n",
    "        print(\"Error fetching news:\", data)\n",
    "        return []\n",
    "\n",
    "# Load FinBERT pipeline\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def analyze_sentiment(texts):\n",
    "    sentiments = sentiment_pipeline(texts, truncation=True)\n",
    "    return [sent[\"label\"] for sent in sentiments]\n",
    "\n",
    "# Combined sentiment fetcher for crude oil-related queries\n",
    "def get_oil_sentiment():\n",
    "    queries = [\n",
    "    \"crude oil\",\n",
    "    \"Brent crude\",\n",
    "    \"WTI\",\n",
    "    \"oil price\",\n",
    "    \"oil futures\",\n",
    "    \"oil market\",\n",
    "    \"oil production\",\n",
    "    \"OPEC\",\n",
    "    \"oil demand\"\n",
    "]\n",
    "    all_articles = []\n",
    "\n",
    "    for query in queries:\n",
    "        articles = fetch_news(query)\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "    if not all_articles:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_articles, columns=[\"Date\", \"Headline\"])\n",
    "    df[\"Sentiment\"] = analyze_sentiment(df[\"Headline\"].tolist())\n",
    "    df.to_csv(\"gold_sentiment_data.csv\", index=False)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5dd1ac-2526-4cbd-ab6b-42ed89b5956d",
   "metadata": {},
   "source": [
    "#### Apply Sentiment analysis on Live data and save them in disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80223850-39d2-4791-a7ba-6aa85f5aef63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Date                                           Headline  \\\n",
      "0  2025-04-30T10:00:45Z            Turkey’s Pivotal Moment With Azerbaijan   \n",
      "1  2025-04-30T10:00:00Z  Trump’s Policies Are Creating Uncertainty for ...   \n",
      "2  2025-04-30T09:53:27Z  Stocks to Buy | Resilience & Rebalancing: Indi...   \n",
      "3  2025-04-30T09:51:38Z         TotalEnergies Profits Drop As Prices Slide   \n",
      "4  2025-04-30T09:38:18Z  HPCL, ADNOC Trading ink their first LNG tradin...   \n",
      "\n",
      "  Sentiment  \n",
      "0   neutral  \n",
      "1  negative  \n",
      "2   neutral  \n",
      "3  negative  \n",
      "4   neutral  \n",
      "                   Date                                           Headline  \\\n",
      "0  2025-04-30T10:00:45Z            Turkey’s Pivotal Moment With Azerbaijan   \n",
      "1  2025-04-30T10:00:00Z  Trump’s Policies Are Creating Uncertainty for ...   \n",
      "2  2025-04-30T09:53:27Z  Stocks to Buy | Resilience & Rebalancing: Indi...   \n",
      "3  2025-04-30T09:51:38Z         TotalEnergies Profits Drop As Prices Slide   \n",
      "4  2025-04-30T09:38:18Z  HPCL, ADNOC Trading ink their first LNG tradin...   \n",
      "\n",
      "  Sentiment  Sentiment_score  \n",
      "0   neutral                0  \n",
      "1  negative               -1  \n",
      "2   neutral                0  \n",
      "3  negative               -1  \n",
      "4   neutral                0  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>Turkey’s Pivotal Moment With Azerbaijan</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>Trump’s Policies Are Creating Uncertainty for ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>Stocks to Buy | Resilience &amp; Rebalancing: Indi...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>TotalEnergies Profits Drop As Prices Slide</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-04-30</td>\n",
       "      <td>HPCL, ADNOC Trading ink their first LNG tradin...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                           Headline Sentiment  \\\n",
       "0  2025-04-30            Turkey’s Pivotal Moment With Azerbaijan   neutral   \n",
       "1  2025-04-30  Trump’s Policies Are Creating Uncertainty for ...  negative   \n",
       "2  2025-04-30  Stocks to Buy | Resilience & Rebalancing: Indi...   neutral   \n",
       "3  2025-04-30         TotalEnergies Profits Drop As Prices Slide  negative   \n",
       "4  2025-04-30  HPCL, ADNOC Trading ink their first LNG tradin...   neutral   \n",
       "\n",
       "   Sentiment_score  \n",
       "0                0  \n",
       "1               -1  \n",
       "2                0  \n",
       "3               -1  \n",
       "4                0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the new data\n",
    "import os\n",
    "Live_Sentiments_file = os.path.join(Live_Sentiments_file_dir, commodity.upper(), Live_Sentiments_file_name)\n",
    "if os.path.exists(Live_Sentiments_file):\n",
    "    # sentiment_df = get_oil_sentiment()\n",
    "    # sentiment_df.to_csv(Live_Sentiments_file, index=False)\n",
    "    sentiment_df =  pd.read_csv(Live_Sentiments_file)  # File does not exist\n",
    "else:\n",
    "    # Fetch sentiment data\n",
    "    sentiment_df = get_oil_sentiment()\n",
    "    sentiment_df.to_csv(Live_Sentiments_file, index=False)\n",
    "print(sentiment_df.head())\n",
    "sentiment_df['Sentiment_score'] = sentiment_df['Sentiment'].apply(lambda x: 1 if x == 'positive' else -1 if x == 'negative' else 0)\n",
    "print(sentiment_df.head())\n",
    "\n",
    "# Add sentiment data back to the news_df\n",
    "sentiment_df[['Date', 'Headline', 'Sentiment', 'Sentiment_score']]\n",
    "live_sentiments_df = sentiment_df[['Date', 'Headline', 'Sentiment', 'Sentiment_score']]\n",
    "# # Print out the resulting dataframe\n",
    "# print(news_df[['Date', 'Title', 'Sentiment']].head())\n",
    "\n",
    "# Convert Date column to datetime and add a range of days around each event\n",
    "live_sentiments_df['Date'] = pd.to_datetime(live_sentiments_df['Date'])\n",
    "\n",
    "\n",
    "\n",
    "live_sentiments_df['Date'] = pd.to_datetime(live_sentiments_df['Date'], errors='coerce')\n",
    "live_sentiments_df['Date'] = live_sentiments_df['Date'].dt.tz_localize(None).dt.date \n",
    "live_sentiments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489c044-d2c6-4e0c-a2ed-f668db19468c",
   "metadata": {},
   "source": [
    "#### Load Historical Sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa81d788-907a-4f90-b833-8221f9166d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# Historical_Sentiments_file = os.path.join(Historical_Sentiments_file_dir, commodity.upper(), Historical_Sentiments_file_name)\n",
    "# #Detect Invalid Dates\n",
    "# def is_invalid_date(date_str):\n",
    "#     try:\n",
    "#         pd.to_datetime(date_str, format=\"%d-%m-%Y\", errors=\"raise\")\n",
    "#         return False  # Valid date\n",
    "#     except:\n",
    "#         return True   # Invalid date\n",
    "\n",
    "\n",
    "\n",
    "# if os.path.exists(Historical_Sentiments_file) and commodity.upper()==\"GOLD\":\n",
    "#         historical_news_df =  pd.read_csv(Historical_Sentiments_file)  # File does not exist\n",
    "#         print(historical_news_df)\n",
    "# elif commodity.upper()==\"GOLD\" :\n",
    "#     # Fetch sentiment data\n",
    "#     historical_sentiment_data = pd.read_csv(\"C:/Users/Lenovo/OneDrive/Desktop/War/code/data/gold_historical_sentiments.csv\", usecols=[\"Dates\",\"Price Sentiment\", \"News\"])\n",
    "#     historical_sentiment_data = historical_sentiment_data[~historical_sentiment_data[\"Dates\"].astype(str).apply(is_invalid_date)]\n",
    "    \n",
    "    \n",
    "#     # Add sentiment data back to the news_df\n",
    "#     # # Print out the resulting dataframe\n",
    "#     # print(news_df[['Date', 'Title', 'Sentiment']].head())\n",
    "#     historical_news_df = historical_sentiment_data[['Dates', 'News', 'Price Sentiment', 'Sentiment_score']]\n",
    "#     historical_news_df.rename(columns = {'News':'Headline', 'Dates':'Date', 'Price Sentiment':'Sentiment'}, inplace = True)\n",
    "    \n",
    "#     # Convert Date column to datetime and add a range of days around each event\n",
    "#     historical_news_df['Date'] = pd.to_datetime(historical_news_df['Date'])\n",
    "#     historical_news_df['Date'] = pd.to_datetime(historical_news_df['Date'], errors='coerce')\n",
    "#     historical_news_df['Date'] = historical_news_df['Date'].dt.tz_localize(None).dt.date \n",
    "\n",
    "#     historical_news_df = historical_news_df[['Date','Headline']]\n",
    "#     historical_news_df[\"Sentiment_score\"] = analyze_sentiment(historical_news_df[\"Headline\"].tolist(), batch_size=16)\n",
    "#     historical_news_df['Sentiment_score'] = historical_news_df['Sentiment_score'].apply(lambda x: 1 if x == 'positive' else -1 if x == 'negative' else 0)\n",
    "#     historical_news_df.to_csv(Historical_Sentiments_file, index=False)\n",
    "# else:\n",
    "#     historical_news_df=live_sentiments_df\n",
    "import pandas as pd\n",
    "import requests\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Define the events\n",
    "events = {\n",
    "    '2003-03-20': 'Iraq War begins, leading to regional instability and oil price increase',\n",
    "    '2011-01-25': 'Arab Spring uprisings cause oil production disruptions',\n",
    "    '2022-02-24': 'Russian invasion of Ukraine impacts oil prices due to sanctions and supply disruptions',\n",
    "    '2008-09-15': 'Aftermath of 2008 Financial Crisis on Oil Prices',\n",
    "    '2024-01-10': 'U.S. Announces New Sanctions on Russia',\n",
    "    '2024-02-15': 'Ukraine Military Counteroffensive',\n",
    "    '2024-03-05': 'Israel-Hamas Conflict Escalation',\n",
    "    '2024-04-12': 'Iran Nuclear Talks Stalled'\n",
    "}\n",
    "\n",
    "# Step 2: Create a DataFrame for the events\n",
    "event_df = pd.DataFrame(list(events.items()), columns=['Date', 'Title'])\n",
    "\n",
    "# Step 3: Load the sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "\n",
    "# Step 4: Perform sentiment analysis on the event titles\n",
    "event_sentiments = sentiment_pipeline(event_df['Title'].tolist())\n",
    "\n",
    "# Step 5: Convert sentiment analysis results to DataFrame\n",
    "sentiment_df = pd.DataFrame(event_sentiments)\n",
    "\n",
    "# Step 6: Map sentiment labels to numerical values\n",
    "event_df['Sentiment'] = sentiment_df['label'].apply(lambda x: 1 if x == 'POSITIVE' else -1 if x == 'NEGATIVE' else 0)\n",
    "# Step 8: Convert 'Date' to datetime for merging\n",
    "\n",
    "event_df['Date'] = pd.to_datetime(event_df['Date'], errors='coerce')\n",
    "event_df['Date'] = event_df['Date'].dt.tz_localize(None).dt.date \n",
    "#gold_data['Date'] = pd.to_datetime(gold_data['Date']).dt.tz_localize(None).dt.date \n",
    "event_df.head()\n",
    "historical_news_df = event_df\n",
    "historical_news_df.rename(columns={'Sentiment':'Sentiment_score'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0a39c-0c32-4385-ab2c-6c8bd74e0227",
   "metadata": {},
   "source": [
    "## Step 2 : Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca0077-123a-4e07-9d82-577fedc4bf70",
   "metadata": {},
   "source": [
    "### 2.1 Null Value Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2a5d3b5-4224-4239-ab04-5779102201d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in [commodity_data]:\n",
    "    data.dropna(subset=['Date', 'Close'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c5ecd8-7e4f-4c06-9124-586fc2718f83",
   "metadata": {},
   "source": [
    "### 2.2 Converting Values to Datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c74c1b1c-0f70-43fe-901d-f8d19cd8d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnvert into date format\n",
    "# Historical Data\n",
    "# Convert Date column in gold_data to remove timezone\n",
    "commodity_data['Date'] = commodity_data['Date'].dt.tz_localize(None).dt.date \n",
    "\n",
    "# Market Economic Data\n",
    "daily_gdp_data['Date'] = daily_gdp_data['Date'].dt.tz_localize(None).dt.date \n",
    "interest_rate_df['Date'] = interest_rate_df['Date'].dt.tz_localize(None).dt.date \n",
    "Inflation_data['Date'] = Inflation_data['Date'].dt.tz_localize(None).dt.date \n",
    "daily_debt_data['Date'] = daily_debt_data['Date'].dt.tz_localize(None).dt.date "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36fa9a-9d8e-4372-91c3-2b056b2315cb",
   "metadata": {},
   "source": [
    "### 2.3 Merging all source data to final data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adecd7a3-6043-4a1a-b658-8ff89e3b7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes on 'Date' with 'gold_data' as the base (left join)\n",
    "merged_data = commodity_data.merge(daily_gdp_data, on='Date', how='left') \\\n",
    "                       .merge(interest_rate_df, on='Date', how='left') \\\n",
    "                       .merge(Inflation_data, on='Date', how='left') \\\n",
    "                       .merge(daily_debt_data, on='Date', how='left') \\\n",
    "                        .merge(etf_data[['ETF_Value']], on='Date', how='left')\n",
    "\n",
    "merged_data.drop(columns=['year','month'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa172db1-2cce-4b66-a3a5-0edcc22f819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5608 entries, 0 to 5607\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Date            5608 non-null   object \n",
      " 1   Open            5608 non-null   float64\n",
      " 2   High            5608 non-null   float64\n",
      " 3   Low             5608 non-null   float64\n",
      " 4   Close           5608 non-null   float64\n",
      " 5   Volume          5608 non-null   int64  \n",
      " 6   GDP             424 non-null    object \n",
      " 7   Interest_rates  172 non-null    float64\n",
      " 8   Inflation_rate  172 non-null    float64\n",
      " 9   DebtToGDP       5027 non-null   float64\n",
      " 10  ETF_Value       142 non-null    float64\n",
      "dtypes: float64(8), int64(1), object(2)\n",
      "memory usage: 482.1+ KB\n",
      "Date                 0\n",
      "Open                 0\n",
      "High                 0\n",
      "Low                  0\n",
      "Close                0\n",
      "Volume               0\n",
      "GDP               5184\n",
      "Interest_rates    5436\n",
      "Inflation_rate    5436\n",
      "DebtToGDP          581\n",
      "ETF_Value         5466\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "merged_data.info()\n",
    "# Print summary of missing values after imputation\n",
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ab45e-9781-455b-ae33-e80eb5458484",
   "metadata": {},
   "source": [
    "### 2.4 Processing NULL values from merged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ae9305d-926c-4e1e-94f6-1fe3b11ac56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date              0\n",
      "Open              0\n",
      "High              0\n",
      "Low               0\n",
      "Close             0\n",
      "Volume            0\n",
      "GDP               0\n",
      "Interest_rates    0\n",
      "Inflation_rate    0\n",
      "DebtToGDP         0\n",
      "ETF_Value         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\3625551758.py:6: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  merged_data[numeric_cols] = merged_data[numeric_cols].fillna(method=\"ffill\")  # Forward Fill (Recommended)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\3625551758.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_data[\"Volume\"].fillna(merged_data[\"Volume\"].median(), inplace=True)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\3625551758.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_data[col].fillna(merged_data[col].mean(), inplace=True)  # Mean Imputation\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\3625551758.py:14: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  merged_data[col].fillna(merged_data[col].mean(), inplace=True)  # Mean Imputation\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\3625551758.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_data[col].fillna(0, inplace=True)  # Mean Imputation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert numeric columns from object type to float (if needed)\n",
    "numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "merged_data[numeric_cols] = merged_data[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Handle Financial Market Data (Time-series features)\n",
    "merged_data[numeric_cols] = merged_data[numeric_cols].fillna(method=\"ffill\")  # Forward Fill (Recommended)\n",
    "\n",
    "#  Handle Volume (Typically Skewed, Use Median)\n",
    "merged_data[\"Volume\"].fillna(merged_data[\"Volume\"].median(), inplace=True)\n",
    "\n",
    "# 🔹 Handle Economic Indicators (Use Mean or Interpolation)\n",
    "econ_cols = [\"GDP\", \"DebtToGDP\", \"Interest_rates\", \"Inflation_rate\", \"ETF_Value\"]\n",
    "for col in econ_cols:\n",
    "    merged_data[col].fillna(merged_data[col].mean(), inplace=True)  # Mean Imputation\n",
    "    merged_data[col].fillna(0, inplace=True)  # Mean Imputation\n",
    "\n",
    "\n",
    "# Print summary of missing values after imputation\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "commodity_data=merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7246f5e-e3b6-4d9c-ba47-954e40c18fb2",
   "metadata": {},
   "source": [
    "### Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e470274-2eb0-4db6-abea-c8ff4d3588f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>ETF_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>2008-06-06</td>\n",
       "      <td>128.199997</td>\n",
       "      <td>139.119995</td>\n",
       "      <td>127.809998</td>\n",
       "      <td>138.539993</td>\n",
       "      <td>508749</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>2008-06-26</td>\n",
       "      <td>134.520004</td>\n",
       "      <td>140.389999</td>\n",
       "      <td>133.679993</td>\n",
       "      <td>139.639999</td>\n",
       "      <td>295773</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>139.440002</td>\n",
       "      <td>142.990005</td>\n",
       "      <td>138.610001</td>\n",
       "      <td>140.210007</td>\n",
       "      <td>277517</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>2008-06-30</td>\n",
       "      <td>140.600006</td>\n",
       "      <td>143.669998</td>\n",
       "      <td>139.169998</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>253428</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>2008-07-01</td>\n",
       "      <td>140.179993</td>\n",
       "      <td>143.330002</td>\n",
       "      <td>139.949997</td>\n",
       "      <td>140.970001</td>\n",
       "      <td>238751</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>2.010000</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>2.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1378</th>\n",
       "      <td>2008-07-02</td>\n",
       "      <td>141.440002</td>\n",
       "      <td>144.320007</td>\n",
       "      <td>140.009995</td>\n",
       "      <td>143.570007</td>\n",
       "      <td>253397</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>144.190002</td>\n",
       "      <td>145.850006</td>\n",
       "      <td>143.220001</td>\n",
       "      <td>145.289993</td>\n",
       "      <td>193841</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1380</th>\n",
       "      <td>2008-07-07</td>\n",
       "      <td>144.270004</td>\n",
       "      <td>144.529999</td>\n",
       "      <td>139.520004</td>\n",
       "      <td>141.369995</td>\n",
       "      <td>304148</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>2008-07-10</td>\n",
       "      <td>135.800003</td>\n",
       "      <td>142.130005</td>\n",
       "      <td>135.429993</td>\n",
       "      <td>141.649994</td>\n",
       "      <td>294339</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>2008-07-11</td>\n",
       "      <td>141.800003</td>\n",
       "      <td>147.270004</td>\n",
       "      <td>141.440002</td>\n",
       "      <td>145.080002</td>\n",
       "      <td>334940</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>2008-07-14</td>\n",
       "      <td>144.690002</td>\n",
       "      <td>146.369995</td>\n",
       "      <td>142.490005</td>\n",
       "      <td>145.179993</td>\n",
       "      <td>252605</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>2008-07-15</td>\n",
       "      <td>145.190002</td>\n",
       "      <td>146.729996</td>\n",
       "      <td>135.919998</td>\n",
       "      <td>138.740005</td>\n",
       "      <td>392225</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>63.815112</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4345</th>\n",
       "      <td>2020-04-20</td>\n",
       "      <td>17.730000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>-40.320000</td>\n",
       "      <td>-37.630001</td>\n",
       "      <td>247947</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>124.733177</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close  Volume  \\\n",
       "1360  2008-06-06  128.199997  139.119995  127.809998  138.539993  508749   \n",
       "1374  2008-06-26  134.520004  140.389999  133.679993  139.639999  295773   \n",
       "1375  2008-06-27  139.440002  142.990005  138.610001  140.210007  277517   \n",
       "1376  2008-06-30  140.600006  143.669998  139.169998  140.000000  253428   \n",
       "1377  2008-07-01  140.179993  143.330002  139.949997  140.970001  238751   \n",
       "1378  2008-07-02  141.440002  144.320007  140.009995  143.570007  253397   \n",
       "1379  2008-07-03  144.190002  145.850006  143.220001  145.289993  193841   \n",
       "1380  2008-07-07  144.270004  144.529999  139.520004  141.369995  304148   \n",
       "1383  2008-07-10  135.800003  142.130005  135.429993  141.649994  294339   \n",
       "1384  2008-07-11  141.800003  147.270004  141.440002  145.080002  334940   \n",
       "1385  2008-07-14  144.690002  146.369995  142.490005  145.179993  252605   \n",
       "1386  2008-07-15  145.190002  146.729996  135.919998  138.740005  392225   \n",
       "4345  2020-04-20   17.730000   17.850000  -40.320000  -37.630001  247947   \n",
       "\n",
       "               GDP  Interest_rates  Inflation_rate   DebtToGDP  ETF_Value  \n",
       "1360  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1374  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1375  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1376  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1377  1.777183e+13        2.010000        2.010000   63.815112   2.250000  \n",
       "1378  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1379  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1380  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1383  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1384  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1385  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "1386  1.777183e+13        1.677791        1.677791   63.815112   1.959507  \n",
       "4345  1.777183e+13        1.677791        1.677791  124.733177   1.959507  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Function to identify and remove outliers using IQR\n",
    "def identify_outliers(df):\n",
    "    # Ensure 'Close' is numeric\n",
    "    df['Close'] = pd.to_numeric(df['Close'], errors='coerce')  # Convert to float, set errors to NaN\n",
    "    df = df.dropna(subset=['Close'])  # Drop rows where 'Close' is NaN after conversion\n",
    "\n",
    "    Q1 = df['Close'].quantile(0.25)\n",
    "    Q3 = df['Close'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    ndf = df[(df['Close'] <= lower_bound) | (df['Close'] >= upper_bound)]\n",
    "    return ndf\n",
    "\n",
    "# Apply outlier detection to the Gold dataset\n",
    "commodity_data['Close'] = pd.to_numeric(commodity_data['Close'], errors='coerce')  # Ensure 'Close' is numeric\n",
    "outlier = identify_outliers(commodity_data)\n",
    "outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9283c04-0b76-42ce-a630-07a4786a1808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>ETF_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>62480</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>68416</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>32.650002</td>\n",
       "      <td>32.849998</td>\n",
       "      <td>31.910000</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>98247</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-07</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>31.080000</td>\n",
       "      <td>124279</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-08</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>108037</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>64.300003</td>\n",
       "      <td>64.419998</td>\n",
       "      <td>62.450001</td>\n",
       "      <td>63.080002</td>\n",
       "      <td>82671</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>2025-04-22</td>\n",
       "      <td>63.430000</td>\n",
       "      <td>65.089996</td>\n",
       "      <td>63.430000</td>\n",
       "      <td>64.309998</td>\n",
       "      <td>297928</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.870003</td>\n",
       "      <td>61.529999</td>\n",
       "      <td>62.270000</td>\n",
       "      <td>397841</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>62.340000</td>\n",
       "      <td>63.310001</td>\n",
       "      <td>61.990002</td>\n",
       "      <td>62.790001</td>\n",
       "      <td>264908</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5608 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Volume  \\\n",
       "0     2003-01-02  31.549999  32.090000  31.400000  31.850000   62480   \n",
       "1     2003-01-03  32.080002  33.200001  31.900000  33.080002   68416   \n",
       "2     2003-01-06  32.650002  32.849998  31.910000  32.099998   98247   \n",
       "3     2003-01-07  31.549999  31.700001  30.510000  31.080000  124279   \n",
       "4     2003-01-08  30.250000  30.700001  29.750000  30.559999  108037   \n",
       "...          ...        ...        ...        ...        ...     ...   \n",
       "5603  2025-04-21  64.300003  64.419998  62.450001  63.080002   82671   \n",
       "5604  2025-04-22  63.430000  65.089996  63.430000  64.309998  297928   \n",
       "5605  2025-04-23  64.000000  64.870003  61.529999  62.270000  397841   \n",
       "5606  2025-04-24  62.340000  63.310001  61.990002  62.790001  264908   \n",
       "5607  2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "\n",
       "               GDP  Interest_rates  Inflation_rate  DebtToGDP  ETF_Value  \n",
       "0     1.145645e+13        1.677791        1.677791  56.250366   1.959507  \n",
       "1     1.145645e+13        1.677791        1.677791  56.250366   1.959507  \n",
       "2     1.145645e+13        1.677791        1.677791  56.250366   1.959507  \n",
       "3     1.145645e+13        1.677791        1.677791  56.250366   1.959507  \n",
       "4     1.145645e+13        1.677791        1.677791  56.250366   1.959507  \n",
       "...            ...             ...             ...        ...        ...  \n",
       "5603  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5604  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5605  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5606  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5607  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "\n",
       "[5608 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434c6a0-dbde-46fd-9979-001a6d98a0f8",
   "metadata": {},
   "source": [
    "###  Data Merging - Sentiments with Commodity Data\n",
    "\n",
    "#### Merging Live and Histrical Sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27e96e5b-d535-4edc-bdf2-3c8c07250192",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_sentiments_df = live_sentiments_df[['Date','Sentiment_score']]\n",
    "historical_news_df = historical_news_df[['Date','Sentiment_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "614c0dc6-097d-4c2a-846e-680f1ab6a920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date  Sentiment_score\n",
      "883 2003-03-20               -1\n",
      "886 2008-09-15               -1\n",
      "884 2011-01-25               -1\n",
      "885 2022-02-24               -1\n",
      "887 2024-01-10               -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\3662196593.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  historical_news_df['Date'] = pd.to_datetime(historical_news_df['Date'])\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column to datetime format in both DataFrames\n",
    "live_sentiments_df['Date'] = pd.to_datetime(live_sentiments_df['Date'])\n",
    "historical_news_df['Date'] = pd.to_datetime(historical_news_df['Date'])\n",
    "\n",
    "# Merge the DataFrames and sort by 'Date'\n",
    "sentiment_df = pd.concat([live_sentiments_df, historical_news_df], ignore_index=True).sort_values(by='Date')\n",
    "\n",
    "# Display the first few rows\n",
    "print(sentiment_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "716ed45f-06e9-4a89-9fbc-f1940c726ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_data['Date'] = pd.to_datetime(commodity_data['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "202f7dfc-27af-49d7-9f5c-475c55231d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>ETF_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5603</th>\n",
       "      <td>2025-04-21</td>\n",
       "      <td>64.300003</td>\n",
       "      <td>64.419998</td>\n",
       "      <td>62.450001</td>\n",
       "      <td>63.080002</td>\n",
       "      <td>82671</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5604</th>\n",
       "      <td>2025-04-22</td>\n",
       "      <td>63.430000</td>\n",
       "      <td>65.089996</td>\n",
       "      <td>63.430000</td>\n",
       "      <td>64.309998</td>\n",
       "      <td>297928</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5605</th>\n",
       "      <td>2025-04-23</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.870003</td>\n",
       "      <td>61.529999</td>\n",
       "      <td>62.270000</td>\n",
       "      <td>397841</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5606</th>\n",
       "      <td>2025-04-24</td>\n",
       "      <td>62.340000</td>\n",
       "      <td>63.310001</td>\n",
       "      <td>61.990002</td>\n",
       "      <td>62.790001</td>\n",
       "      <td>264908</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5607</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Open       High        Low      Close  Volume  \\\n",
       "5603 2025-04-21  64.300003  64.419998  62.450001  63.080002   82671   \n",
       "5604 2025-04-22  63.430000  65.089996  63.430000  64.309998  297928   \n",
       "5605 2025-04-23  64.000000  64.870003  61.529999  62.270000  397841   \n",
       "5606 2025-04-24  62.340000  63.310001  61.990002  62.790001  264908   \n",
       "5607 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "\n",
       "               GDP  Interest_rates  Inflation_rate  DebtToGDP  ETF_Value  \n",
       "5603  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5604  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5605  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5606  1.777183e+13        1.677791        1.677791  86.166459   1.959507  \n",
       "5607  1.777183e+13        1.677791        1.677791  86.166459   1.959507  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce208bad-a430-4f82-adaa-b677d03b8d79",
   "metadata": {},
   "source": [
    "### 2.8 Merging Sentiment Data with Gold Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b3e5497-3e92-4761-930e-00ce61374195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>ETF_Value</th>\n",
       "      <th>Sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>31.40</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>62480</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.90</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>68416</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>32.650002</td>\n",
       "      <td>32.849998</td>\n",
       "      <td>31.91</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>98247</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-07</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>30.51</td>\n",
       "      <td>31.080000</td>\n",
       "      <td>124279</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-08</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>29.75</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>108037</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date       Open       High    Low      Close  Volume           GDP  \\\n",
       "0 2003-01-02  31.549999  32.090000  31.40  31.850000   62480  1.145645e+13   \n",
       "1 2003-01-03  32.080002  33.200001  31.90  33.080002   68416  1.145645e+13   \n",
       "2 2003-01-06  32.650002  32.849998  31.91  32.099998   98247  1.145645e+13   \n",
       "3 2003-01-07  31.549999  31.700001  30.51  31.080000  124279  1.145645e+13   \n",
       "4 2003-01-08  30.250000  30.700001  29.75  30.559999  108037  1.145645e+13   \n",
       "\n",
       "   Interest_rates  Inflation_rate  DebtToGDP  ETF_Value  Sentiment_score  \n",
       "0        1.677791        1.677791  56.250366   1.959507              NaN  \n",
       "1        1.677791        1.677791  56.250366   1.959507              NaN  \n",
       "2        1.677791        1.677791  56.250366   1.959507              NaN  \n",
       "3        1.677791        1.677791  56.250366   1.959507              NaN  \n",
       "4        1.677791        1.677791  56.250366   1.959507              NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data = pd.merge(commodity_data, sentiment_df[['Date', 'Sentiment_score']], on='Date', how='left')\n",
    "commodity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "014e550d-355e-4a9f-89fc-b5dd049669ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment_score\n",
       "-1.0    58\n",
       " 0.0    41\n",
       " 1.0    25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data['Sentiment_score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bbf16-b900-4d6b-80e2-b86329b9aee4",
   "metadata": {},
   "source": [
    "### 2.9 Null Treatment post sentiment merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe6db6d5-5994-4a96-b6d2-fc9d35d89a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date               0\n",
      "Open               0\n",
      "High               0\n",
      "Low                0\n",
      "Close              0\n",
      "Volume             0\n",
      "GDP                0\n",
      "Interest_rates     0\n",
      "Inflation_rate     0\n",
      "DebtToGDP          0\n",
      "ETF_Value          0\n",
      "Sentiment_score    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_27916\\1915627315.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  commodity_data['Sentiment_score'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Interpolation – Estimate Sentiment Values Based on Nearby Data\n",
    "#Good for gradual sentiment changes but may not capture sudden shifts.\n",
    "commodity_data['Sentiment_score'] = sentiment_df['Sentiment_score'].interpolate(method='linear')  \n",
    "\n",
    "commodity_data['Sentiment_score'].fillna(0, inplace=True)\n",
    "# Print summary of missing values after imputation\n",
    "print(commodity_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a17ec0e4-14dc-4daa-a785-23c30807c9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment_score\n",
       " 0.0    5209\n",
       "-1.0     286\n",
       " 1.0     224\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data['Sentiment_score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b0b9652-9fde-491a-879d-b02c76a40d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>ETF_Value</th>\n",
       "      <th>Sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>62480</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>68416</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>32.650002</td>\n",
       "      <td>32.849998</td>\n",
       "      <td>31.910000</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>98247</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-07</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>31.080000</td>\n",
       "      <td>124279</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-08</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>108037</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5719 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Open       High        Low      Close  Volume  \\\n",
       "0    2003-01-02  31.549999  32.090000  31.400000  31.850000   62480   \n",
       "1    2003-01-03  32.080002  33.200001  31.900000  33.080002   68416   \n",
       "2    2003-01-06  32.650002  32.849998  31.910000  32.099998   98247   \n",
       "3    2003-01-07  31.549999  31.700001  30.510000  31.080000  124279   \n",
       "4    2003-01-08  30.250000  30.700001  29.750000  30.559999  108037   \n",
       "...         ...        ...        ...        ...        ...     ...   \n",
       "5714 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5715 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5716 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5717 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5718 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "\n",
       "               GDP  Interest_rates  Inflation_rate  DebtToGDP  ETF_Value  \\\n",
       "0     1.145645e+13        1.677791        1.677791  56.250366   1.959507   \n",
       "1     1.145645e+13        1.677791        1.677791  56.250366   1.959507   \n",
       "2     1.145645e+13        1.677791        1.677791  56.250366   1.959507   \n",
       "3     1.145645e+13        1.677791        1.677791  56.250366   1.959507   \n",
       "4     1.145645e+13        1.677791        1.677791  56.250366   1.959507   \n",
       "...            ...             ...             ...        ...        ...   \n",
       "5714  1.777183e+13        1.677791        1.677791  86.166459   1.959507   \n",
       "5715  1.777183e+13        1.677791        1.677791  86.166459   1.959507   \n",
       "5716  1.777183e+13        1.677791        1.677791  86.166459   1.959507   \n",
       "5717  1.777183e+13        1.677791        1.677791  86.166459   1.959507   \n",
       "5718  1.777183e+13        1.677791        1.677791  86.166459   1.959507   \n",
       "\n",
       "      Sentiment_score  \n",
       "0                 0.0  \n",
       "1                -1.0  \n",
       "2                 0.0  \n",
       "3                -1.0  \n",
       "4                 0.0  \n",
       "...               ...  \n",
       "5714              0.0  \n",
       "5715              0.0  \n",
       "5716              0.0  \n",
       "5717              0.0  \n",
       "5718              0.0  \n",
       "\n",
       "[5719 rows x 12 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02e881-de27-4fad-9de3-0e4a209018be",
   "metadata": {},
   "source": [
    "## Step 3 : Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9437eab3-4c66-484a-b4e7-4c991396d51d",
   "metadata": {},
   "source": [
    "### 4.1 Create Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b28f4de-4d34-450f-b229-ae25e6d5c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def feature_engineering(df, world_bank_data=None):\n",
    "    # Lagged values and moving averages\n",
    "    df['lag_1'] = df['Close'].shift(1)\n",
    "    df['lag_7'] = df['Close'].shift(7)\n",
    "    df['lag_30'] = df['Close'].shift(30)\n",
    "    df['MA_7'] = df['Close'].rolling(window=7).mean()\n",
    "    df['MA_30'] = df['Close'].rolling(window=30).mean()\n",
    "    df['MA_90'] = df['Close'].rolling(window=90).mean()\n",
    "    df['tomorrow_price'] = df['Close'].shift(-1)\n",
    "    # Compute log returns fro GARCH\n",
    "    df['Log_Returns'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "\n",
    "    # Adding volatility features\n",
    "    df['volatility_7'] = df['Close'].rolling(window=7).std()\n",
    "    df['volatility_30'] = df['Close'].rolling(window=30).std()\n",
    "\n",
    "    # #Gold prices may respond more to cumulative sentiment trends rather than individual daily sentiments. We apply a moving average to smooth fluctuations.\n",
    "    # # Creating lagged sentiment features\n",
    "    # df[\"Sentiment_Lag_1\"] = df[\"Sentiment_score\"].shift(1)\n",
    "    # df[\"Sentiment_Lag_3\"] = df[\"Sentiment_score\"].shift(3)\n",
    "    # df[\"Sentiment_Lag_7\"] = df[\"Sentiment_score\"].shift(7)\n",
    "    # Compute rolling mean sentiment over 14-day and 30-day windows\n",
    "    df[\"Sentiment_Rolling_14\"] = df[\"Sentiment_score\"].rolling(window=14, min_periods=1).mean()\n",
    "    df[\"Sentiment_Rolling_30\"] = df[\"Sentiment_score\"].rolling(window=30, min_periods=1).mean()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157679fd-25f6-4d80-a363-df324462c509",
   "metadata": {},
   "source": [
    "### 4.2 Apply Feature engineering on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bded4d6-6b3c-4c2f-bceb-d1ca6cd7cfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "commodity_data = feature_engineering(commodity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aae9fcef-cdde-4328-be3a-83a91cc2f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_30</th>\n",
       "      <th>MA_7</th>\n",
       "      <th>MA_30</th>\n",
       "      <th>MA_90</th>\n",
       "      <th>tomorrow_price</th>\n",
       "      <th>Log_Returns</th>\n",
       "      <th>volatility_7</th>\n",
       "      <th>volatility_30</th>\n",
       "      <th>Sentiment_Rolling_14</th>\n",
       "      <th>Sentiment_Rolling_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-01-02</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>32.090000</td>\n",
       "      <td>31.400000</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>62480</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-01-03</td>\n",
       "      <td>32.080002</td>\n",
       "      <td>33.200001</td>\n",
       "      <td>31.900000</td>\n",
       "      <td>33.080002</td>\n",
       "      <td>68416</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>0.037892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-01-06</td>\n",
       "      <td>32.650002</td>\n",
       "      <td>32.849998</td>\n",
       "      <td>31.910000</td>\n",
       "      <td>32.099998</td>\n",
       "      <td>98247</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.080000</td>\n",
       "      <td>-0.030073</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-01-07</td>\n",
       "      <td>31.549999</td>\n",
       "      <td>31.700001</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>31.080000</td>\n",
       "      <td>124279</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>-0.032291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-01-08</td>\n",
       "      <td>30.250000</td>\n",
       "      <td>30.700001</td>\n",
       "      <td>29.750000</td>\n",
       "      <td>30.559999</td>\n",
       "      <td>108037</td>\n",
       "      <td>1.145645e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>56.250366</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.990000</td>\n",
       "      <td>-0.016873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.892445</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.878112</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5716</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.863778</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.872112</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>2025-04-25</td>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.410000</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.020000</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.880445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5719 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date       Open       High        Low      Close  Volume  \\\n",
       "0    2003-01-02  31.549999  32.090000  31.400000  31.850000   62480   \n",
       "1    2003-01-03  32.080002  33.200001  31.900000  33.080002   68416   \n",
       "2    2003-01-06  32.650002  32.849998  31.910000  32.099998   98247   \n",
       "3    2003-01-07  31.549999  31.700001  30.510000  31.080000  124279   \n",
       "4    2003-01-08  30.250000  30.700001  29.750000  30.559999  108037   \n",
       "...         ...        ...        ...        ...        ...     ...   \n",
       "5714 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5715 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5716 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5717 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "5718 2025-04-25  62.860001  63.410000  61.799999  63.020000  283758   \n",
       "\n",
       "               GDP  Interest_rates  Inflation_rate  DebtToGDP  ...  lag_30  \\\n",
       "0     1.145645e+13        1.677791        1.677791  56.250366  ...     NaN   \n",
       "1     1.145645e+13        1.677791        1.677791  56.250366  ...     NaN   \n",
       "2     1.145645e+13        1.677791        1.677791  56.250366  ...     NaN   \n",
       "3     1.145645e+13        1.677791        1.677791  56.250366  ...     NaN   \n",
       "4     1.145645e+13        1.677791        1.677791  56.250366  ...     NaN   \n",
       "...            ...             ...             ...        ...  ...     ...   \n",
       "5714  1.777183e+13        1.677791        1.677791  86.166459  ...   63.02   \n",
       "5715  1.777183e+13        1.677791        1.677791  86.166459  ...   63.02   \n",
       "5716  1.777183e+13        1.677791        1.677791  86.166459  ...   63.02   \n",
       "5717  1.777183e+13        1.677791        1.677791  86.166459  ...   63.02   \n",
       "5718  1.777183e+13        1.677791        1.677791  86.166459  ...   63.02   \n",
       "\n",
       "       MA_7  MA_30      MA_90  tomorrow_price  Log_Returns  volatility_7  \\\n",
       "0       NaN    NaN        NaN       33.080002          NaN           NaN   \n",
       "1       NaN    NaN        NaN       32.099998     0.037892           NaN   \n",
       "2       NaN    NaN        NaN       31.080000    -0.030073           NaN   \n",
       "3       NaN    NaN        NaN       30.559999    -0.032291           NaN   \n",
       "4       NaN    NaN        NaN       31.990000    -0.016873           NaN   \n",
       "...     ...    ...        ...             ...          ...           ...   \n",
       "5714  63.02  63.02  62.892445       63.020000     0.000000           0.0   \n",
       "5715  63.02  63.02  62.878112       63.020000     0.000000           0.0   \n",
       "5716  63.02  63.02  62.863778       63.020000     0.000000           0.0   \n",
       "5717  63.02  63.02  62.872112       63.020000     0.000000           0.0   \n",
       "5718  63.02  63.02  62.880445             NaN     0.000000           0.0   \n",
       "\n",
       "      volatility_30  Sentiment_Rolling_14  Sentiment_Rolling_30  \n",
       "0               NaN              0.000000              0.000000  \n",
       "1               NaN             -0.500000             -0.500000  \n",
       "2               NaN             -0.333333             -0.333333  \n",
       "3               NaN             -0.500000             -0.500000  \n",
       "4               NaN             -0.400000             -0.400000  \n",
       "...             ...                   ...                   ...  \n",
       "5714            0.0              0.000000              0.000000  \n",
       "5715            0.0              0.000000              0.000000  \n",
       "5716            0.0              0.000000              0.000000  \n",
       "5717            0.0              0.000000              0.000000  \n",
       "5718            0.0              0.000000              0.000000  \n",
       "\n",
       "[5719 rows x 24 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_engineering(commodity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b6a4df5-7e2c-48ec-951c-25b8a4093f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Close</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>0.998976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>0.998339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>0.997174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_1</th>\n",
       "      <td>0.996619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tomorrow_price</th>\n",
       "      <td>0.996619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MA_7</th>\n",
       "      <td>0.995315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_7</th>\n",
       "      <td>0.983372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MA_30</th>\n",
       "      <td>0.977479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag_30</th>\n",
       "      <td>0.927650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MA_90</th>\n",
       "      <td>0.914439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility_30</th>\n",
       "      <td>0.211080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volatility_7</th>\n",
       "      <td>0.190818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Rolling_30</th>\n",
       "      <td>0.126744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <td>0.112826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DebtToGDP</th>\n",
       "      <td>0.090274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_Rolling_14</th>\n",
       "      <td>0.087955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GDP</th>\n",
       "      <td>0.048039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment_score</th>\n",
       "      <td>0.032852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Log_Returns</th>\n",
       "      <td>0.016697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inflation_rate</th>\n",
       "      <td>0.004238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interest_rates</th>\n",
       "      <td>0.004238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ETF_Value</th>\n",
       "      <td>-0.015054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>-0.104806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Correlation\n",
       "Close                    1.000000\n",
       "Low                      0.998976\n",
       "High                     0.998339\n",
       "Open                     0.997174\n",
       "lag_1                    0.996619\n",
       "tomorrow_price           0.996619\n",
       "MA_7                     0.995315\n",
       "lag_7                    0.983372\n",
       "MA_30                    0.977479\n",
       "lag_30                   0.927650\n",
       "MA_90                    0.914439\n",
       "volatility_30            0.211080\n",
       "volatility_7             0.190818\n",
       "Sentiment_Rolling_30     0.126744\n",
       "Date                     0.112826\n",
       "DebtToGDP                0.090274\n",
       "Sentiment_Rolling_14     0.087955\n",
       "GDP                      0.048039\n",
       "Sentiment_score          0.032852\n",
       "Log_Returns              0.016697\n",
       "Inflation_rate           0.004238\n",
       "Interest_rates           0.004238\n",
       "ETF_Value               -0.015054\n",
       "Volume                  -0.104806"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation of all features with 'Close'\n",
    "correlation_with_close = commodity_data.corr()[\"Close\"].sort_values(ascending=False)\n",
    "\n",
    "# Convert to DataFrame\n",
    "correlation_df = correlation_with_close.to_frame().rename(columns={\"Close\": \"Correlation\"})\n",
    "\n",
    "correlation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd49b4c-9ce0-465f-b6a8-0f9bdc0259ec",
   "metadata": {},
   "source": [
    "### 4.3 Seting Index of data frame is as a Date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "395ea7c4-58e6-4ea2-a894-dfdcad0d5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_data_df = commodity_data\n",
    "commodity_data_df.set_index('Date', drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7d53386-1c67-45a5-80f7-632ec67c61cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Interest_rates</th>\n",
       "      <th>Inflation_rate</th>\n",
       "      <th>DebtToGDP</th>\n",
       "      <th>ETF_Value</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_30</th>\n",
       "      <th>MA_7</th>\n",
       "      <th>MA_30</th>\n",
       "      <th>MA_90</th>\n",
       "      <th>tomorrow_price</th>\n",
       "      <th>Log_Returns</th>\n",
       "      <th>volatility_7</th>\n",
       "      <th>volatility_30</th>\n",
       "      <th>Sentiment_Rolling_14</th>\n",
       "      <th>Sentiment_Rolling_30</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-04-25</th>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.02</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.892445</td>\n",
       "      <td>63.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-25</th>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.02</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.878112</td>\n",
       "      <td>63.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-25</th>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.02</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.863778</td>\n",
       "      <td>63.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-25</th>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.02</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.872112</td>\n",
       "      <td>63.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-04-25</th>\n",
       "      <td>62.860001</td>\n",
       "      <td>63.41</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>63.02</td>\n",
       "      <td>283758</td>\n",
       "      <td>1.777183e+13</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>1.677791</td>\n",
       "      <td>86.166459</td>\n",
       "      <td>1.959507</td>\n",
       "      <td>...</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.880445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open   High        Low  Close  Volume           GDP  \\\n",
       "Date                                                                   \n",
       "2025-04-25  62.860001  63.41  61.799999  63.02  283758  1.777183e+13   \n",
       "2025-04-25  62.860001  63.41  61.799999  63.02  283758  1.777183e+13   \n",
       "2025-04-25  62.860001  63.41  61.799999  63.02  283758  1.777183e+13   \n",
       "2025-04-25  62.860001  63.41  61.799999  63.02  283758  1.777183e+13   \n",
       "2025-04-25  62.860001  63.41  61.799999  63.02  283758  1.777183e+13   \n",
       "\n",
       "            Interest_rates  Inflation_rate  DebtToGDP  ETF_Value  ...  lag_30  \\\n",
       "Date                                                              ...           \n",
       "2025-04-25        1.677791        1.677791  86.166459   1.959507  ...   63.02   \n",
       "2025-04-25        1.677791        1.677791  86.166459   1.959507  ...   63.02   \n",
       "2025-04-25        1.677791        1.677791  86.166459   1.959507  ...   63.02   \n",
       "2025-04-25        1.677791        1.677791  86.166459   1.959507  ...   63.02   \n",
       "2025-04-25        1.677791        1.677791  86.166459   1.959507  ...   63.02   \n",
       "\n",
       "             MA_7  MA_30      MA_90  tomorrow_price  Log_Returns  \\\n",
       "Date                                                               \n",
       "2025-04-25  63.02  63.02  62.892445           63.02          0.0   \n",
       "2025-04-25  63.02  63.02  62.878112           63.02          0.0   \n",
       "2025-04-25  63.02  63.02  62.863778           63.02          0.0   \n",
       "2025-04-25  63.02  63.02  62.872112           63.02          0.0   \n",
       "2025-04-25  63.02  63.02  62.880445             NaN          0.0   \n",
       "\n",
       "            volatility_7  volatility_30  Sentiment_Rolling_14  \\\n",
       "Date                                                            \n",
       "2025-04-25           0.0            0.0                   0.0   \n",
       "2025-04-25           0.0            0.0                   0.0   \n",
       "2025-04-25           0.0            0.0                   0.0   \n",
       "2025-04-25           0.0            0.0                   0.0   \n",
       "2025-04-25           0.0            0.0                   0.0   \n",
       "\n",
       "            Sentiment_Rolling_30  \n",
       "Date                              \n",
       "2025-04-25                   0.0  \n",
       "2025-04-25                   0.0  \n",
       "2025-04-25                   0.0  \n",
       "2025-04-25                   0.0  \n",
       "2025-04-25                   0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commodity_data_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be330a1c-1540-407f-876e-c00977c51d43",
   "metadata": {},
   "source": [
    "### Step 5 Data Split and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1047ca42-6c7e-4bea-a404-bc33edb096ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (4576, 23), Test size: (1144, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert index to datetime if not already\n",
    "commodity_data_df.index = pd.to_datetime(commodity_data_df.index)\n",
    "\n",
    "# Define 80-20% split based on row count\n",
    "split_index = int(len(commodity_data_df) * 0.80) \n",
    "# split_date =  pd.to_datetime('2022-01-01')\n",
    "split_date = commodity_data_df.index[split_index]  \n",
    "\n",
    "# Train-test split\n",
    "train_df = commodity_data_df.loc[:split_date]  \n",
    "test_df = commodity_data_df.loc[split_date:]\n",
    "\n",
    "print(f\"Train size: {train_df.shape}, Test size: {test_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aad8f244-64a3-4f97-8d2c-e7b26678b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "\n",
    "# Select features and target\n",
    "X_train = train_df.drop(columns=['tomorrow_price', 'Close' ])\n",
    "y_train = train_df['Close']\n",
    "X_test = test_df.drop(columns=['tomorrow_price', 'Close'])\n",
    "y_test = test_df['Close']\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dba8efe4-e10a-4e92-b76c-b8d5f4d5f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Define scalers\n",
    "scaler_minmax = MinMaxScaler()  # For XGBoost, Random Forest\n",
    "scaler_standard = StandardScaler()  # For LSTM\n",
    "\n",
    "# Apply MinMax Scaling (for tree-based models)\n",
    "X_train_scaled_minmax = scaler_minmax.fit_transform(X_train)\n",
    "X_test_scaled_minmax = scaler_minmax.transform(X_test)\n",
    "\n",
    "# Apply Standard Scaling (for LSTM)\n",
    "X_train_scaled_standard = scaler_standard.fit_transform(X_train)\n",
    "X_test_scaled_standard = scaler_standard.transform(X_test)\n",
    "\n",
    "\n",
    "# # Rescale y_train\n",
    "# scaler_y = MinMaxScaler()\n",
    "y_train_scaled = scaler_minmax.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n",
    "y_test_scaled = scaler_minmax.transform(y_test.values.reshape(-1, 1)).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e5b80028-820e-4648-9955-c2597a05ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model directory\n",
    "# model_dir = hybrid_models_dir.get(commodity.upper())\n",
    "\n",
    "# # Define the model directory\n",
    "# meta_dir = hybrid_meta_models_dir.get(commodity.upper())\n",
    "# hybridmodel_name = 'meta_model_'+commodity+'.pkl'\n",
    "# hybrid_meta_model_name = os.path.join(meta_dir, hybridmodel_name)\n",
    "# # Load the meta-model (used in stacked hybrid)\n",
    "# with open(hybrid_meta_model_name, 'rb') as f:\n",
    "#     meta_model = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35ec0307-8b9a-4367-a8af-059877d4b771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to load models\n",
    "# def load_saved_models(directory):\n",
    "#     model_files = [f for f in os.listdir(directory) if f.endswith('.pkl')]\n",
    "#     models = {}\n",
    "#     for file in model_files:\n",
    "#         model_path = os.path.join(directory, file)\n",
    "#         model_name = file.replace(\".pkl\", \"\")\n",
    "#         print(model_path)\n",
    "#         model = joblib.load(model_path)\n",
    "#         models[model_name] = model\n",
    "#     return models\n",
    "\n",
    "# # Load all saved models\n",
    "# loaded_models = load_saved_models(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621cef0-f0b8-45e6-932a-5b34c260f416",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 8.1 Load pre trained model from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "35bd063f-38c5-40bf-bdc6-4eacb1521288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting using model: RF_best-2\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:413: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0` or remove this parameter as it is also the default value for RandomForestRegressors and ExtraTreesRegressors.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [60.1200002  61.2083327  61.47499943 ... 62.11749935 62.11749935\n",
      " 62.11749935]\n",
      "\n",
      "Predicting using model: XGBoost_best-1\n",
      "Predicted values: [59.948647 61.30818  61.375725 ... 62.633995 62.633995 62.633995]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the model directory\n",
    "model_dir = hybrid_models_dir.get(commodity.upper())\n",
    "\n",
    "# Define the model directory\n",
    "meta_dir = hybrid_meta_models_dir.get(commodity.upper())\n",
    "hybridmodel_name = 'meta_model_'+commodity+'.pkl'\n",
    "hybrid_meta_model_name = os.path.join(meta_dir, hybridmodel_name)\n",
    "# Load the meta-model (used in stacked hybrid)\n",
    "with open(hybrid_meta_model_name, 'rb') as f:\n",
    "    meta_model = pickle.load(f)\n",
    "\n",
    "# Function to load models\n",
    "def load_saved_models(directory):\n",
    "    model_files = [f for f in os.listdir(directory) if f.endswith('.pkl')]\n",
    "    models = {}\n",
    "    for file in model_files:\n",
    "        model_path = os.path.join(directory, file)\n",
    "        model_name = file.replace(\".pkl\", \"\")\n",
    "        model = joblib.load(model_path)\n",
    "        models[model_name] = model\n",
    "    return models\n",
    "\n",
    "# Load all saved models\n",
    "loaded_models = load_saved_models(model_dir)\n",
    "\n",
    "\n",
    "def predict_from_model(model_name, model):\n",
    "    if \"XGBOOST\" in model_name.upper():\n",
    "        model = model.fit(X_train_scaled_minmax, y_train_scaled)\n",
    "        xgb_predictions = model.predict(X_test_scaled_minmax)\n",
    "        xgb_prediction = scaler_minmax.inverse_transform(xgb_predictions.reshape(-1, 1)).ravel()\n",
    "        y_test_org = scaler_minmax.inverse_transform(y_train_scaled.reshape(-1, 1)).ravel()\n",
    "        return scaler_minmax.inverse_transform(xgb_predictions.reshape(-1, 1)).ravel()\n",
    "\n",
    "    elif \"RF\" in model_name.upper():\n",
    "        #  Remove Sentiment & Macroeconomic Features from Training Data\n",
    "        exclude_columns = ['GDP', 'Interest_rates','Inflation_rate', 'DebtToGDP', 'ETF_Value', 'Sentiment_score', 'Sentiment_Rolling_14',\n",
    "        'Sentiment_Rolling_30', 'Sentiment_smoothed']\n",
    "        X_train_filtered = pd.DataFrame(X_train_scaled_minmax, columns=X_train.columns).drop(columns=exclude_columns, errors='ignore').fillna(0.0)\n",
    "        X_test_filtered = pd.DataFrame(X_test_scaled_minmax, columns=X_test.columns).drop(columns=exclude_columns, errors='ignore').fillna(0.0)\n",
    "\n",
    "        random_search = model.fit(X_train_filtered, y_train_scaled)\n",
    "        y_pred_rf = random_search.predict(X_test_filtered)\n",
    "        return  scaler_minmax.inverse_transform(y_pred_rf.reshape(-1, 1)).ravel()\n",
    "\n",
    "    elif \"LSTM\" in model_name.upper():\n",
    "        #  Select Sentiment + Microeconomic Features\n",
    "        selected_features = [\n",
    "            'Open', 'High', 'Low', 'Volume', 'lag_1', 'lag_7', 'lag_30',\n",
    "            'MA_7', 'MA_30', 'MA_90', 'Log_Returns', 'volatility_7', 'volatility_30',\n",
    "            'Sentiment_score', 'Sentiment_Rolling_14', 'Sentiment_Rolling_30'\n",
    "        ]\n",
    "        \n",
    "        # Extract Features for Training & Testing\n",
    "        X_train_selected = pd.DataFrame(X_train_scaled_standard, columns=X_train.columns)[selected_features].values\n",
    "        X_test_selected = pd.DataFrame(X_test_scaled_standard, columns=X_test.columns)[selected_features].values\n",
    "        \n",
    "        #  Reshape Data for LSTM (3D: Samples, Time Steps, Features)\n",
    "        look_back = 14  # Using the same look-back period\n",
    "        \n",
    "        def create_sequences(X, y, look_back):\n",
    "            Xs, ys = [], []\n",
    "            for i in range(len(X) - look_back):\n",
    "                Xs.append(X[i:i + look_back])\n",
    "                ys.append(y[i + look_back])\n",
    "            return np.array(Xs), np.array(ys)\n",
    "        \n",
    "        X_train_seq, y_train_seq = create_sequences(X_train_selected, y_train_scaled, look_back)\n",
    "        X_test_seq, y_test_seq = create_sequences(X_test_selected, y_test_scaled, look_back)\n",
    "        #  Train Model\n",
    "        model = model.fit(\n",
    "            X_train_seq, y_train_seq,\n",
    "            epochs=50, batch_size=32, \n",
    "            validation_data=(X_test_seq, y_test_seq),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        #  Predict on Test Set\n",
    "        y_pred_lstm_scaled = model.predict(X_test_seq)\n",
    "        \n",
    "        #  Inverse Transform Predictions to Original Scale\n",
    "        y_pred_lstm_original = scaler_minmax.inverse_transform(y_pred_lstm_scaled)\n",
    "        return y_pred_lstm_original\n",
    "\n",
    "    elif \"ARIMA\" in model_name.upper(): \n",
    "        # Select microeconomic factors from the scaled data\n",
    "        exog_train = X_train_scaled_minmax[:, [X_train.columns.get_loc(col) for col in ['Interest_rates', 'Inflation_rate', 'GDP', 'DebtToGDP', 'ETF_Value','Sentiment_score']]]\n",
    "        exog_test = X_test_scaled_minmax[:, [X_test.columns.get_loc(col) for col in ['Interest_rates', 'Inflation_rate', 'GDP', 'DebtToGDP', 'ETF_Value','Sentiment_score']]]\n",
    "        model2 = model.get_forecast(steps=len(y_test), exog=exog_test)\n",
    "        fc_model2 = model2.predicted_mean\n",
    "        return fc_model2.values\n",
    "        \n",
    "    elif \"GARCH\" in model_name.upper():\n",
    "        y_train_log_returns = np.log(y_train / y_train.shift(1)).dropna()\n",
    "        y_test_log_returns = np.log(y_test / y_test.shift(1)).dropna()\n",
    "        garch_forecast = model.forecast(horizon=len(y_test_log_returns))\n",
    "        garch_forecast_series = garch_forecast.mean.iloc[-len(y_test_log_returns):].values.flatten()\n",
    "        # Convert forecasted log returns back to price predictions\n",
    "        y_test_prices = y_test.iloc[-len(garch_forecast_series):].values\n",
    "        garch_predicted_prices = y_test_prices[0] * np.exp(np.cumsum(garch_forecast_series))\n",
    "        return garch_predicted_prices\n",
    "        \n",
    "    elif \"VAR\" in model_name.upper():\n",
    "        # Ensure data is stationary (apply differencing)\n",
    "        train_var = train_df[['Close', 'Volume', 'MA_7', 'MA_30', 'MA_90', 'Log_Returns', \n",
    "                              'volatility_7', 'volatility_30', 'Sentiment_Rolling_14', 'Sentiment_Rolling_30']].diff().dropna()\n",
    "        test_var = test_df[['Close', 'Volume', 'MA_7', 'MA_30', 'MA_90', 'Log_Returns', \n",
    "                            'volatility_7', 'volatility_30', 'Sentiment_Rolling_14', 'Sentiment_Rolling_30']].diff().dropna()\n",
    "        \n",
    "        # train_var = train_df[['Close', 'Volume', 'MA_7', 'MA_30']].diff().dropna()\n",
    "        # test_var = test_df[['Close', 'Volume', 'MA_7', 'MA_30']].diff().dropna()\n",
    "        train_exog = train_df[['GDP', 'Interest_rates', 'Inflation_rate', 'DebtToGDP', 'ETF_Value']].iloc[1:]\n",
    "        test_exog = test_df[['GDP', 'Interest_rates', 'Inflation_rate', 'DebtToGDP', 'ETF_Value',]].iloc[1:]\n",
    "        \n",
    "        # train_exog = train_df[['GDP', 'Interest_rates', 'Inflation_rate', 'ETF_Value']].iloc[1:]  # Align with differencing\n",
    "        # test_exog = test_df[['GDP', 'Interest_rates', 'Inflation_rate', 'ETF_Value']].iloc[1:]    # Align with differencing\n",
    "        \n",
    "        # Select optimal lag using BIC\n",
    "        var_model = VAR(endog=train_var, exog=train_exog)\n",
    "        lag_selection = var_model.select_order(maxlags=15)\n",
    "        optimal_lag = lag_selection.bic\n",
    "        print(f\"Optimal lag order (BIC): {optimal_lag}\")\n",
    "        \n",
    "        # Fit VAR model\n",
    "        var_fitted = var_model.fit(optimal_lag)\n",
    "        \n",
    "        # Forecast\n",
    "        forecast_steps = len(test_var)\n",
    "        last_observations = train_var.values[-optimal_lag:]\n",
    "        \n",
    "        # Forecast with exogenous variables\n",
    "        var_forecast = var_fitted.forecast(last_observations, steps=forecast_steps, exog_future=test_exog)\n",
    "        \n",
    "        # Convert forecasted differenced values back to original scale\n",
    "        forecast_df_m4 = pd.DataFrame(var_forecast, columns=train_var.columns, index=test_var.index)\n",
    "        forecast_df_m4 = train_df[['Close', 'Volume', 'MA_7', 'MA_30', 'MA_90', 'Log_Returns', 'volatility_7', 'volatility_30', 'Sentiment_Rolling_14', 'Sentiment_Rolling_30']].iloc[-1] + forecast_df_m4.cumsum()\n",
    "        \n",
    "        # Extract predicted Close prices\n",
    "        predicted_prices = forecast_df_m4['Close']\n",
    "        return predicted_prices\n",
    "    else:\n",
    "        print(f\"Unknown model type for {model_name}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "predict_d= {}\n",
    "for model_name, model in loaded_models.items():\n",
    "    print(f\"\\nPredicting using model: {model_name}\")\n",
    "    prediction = predict_from_model(model_name, model)\n",
    "    predict_d[model_name]=prediction\n",
    "    if prediction is not None:\n",
    "        print(\"Predicted values:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5e3e0-754b-4722-bd8f-17316db85599",
   "metadata": {},
   "source": [
    "### 8.2 Load Pretrained Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "faf91b77-5ea3-4f99-bc43-207e0934817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "best1_forecast_trimmed,best2_forecast_trimmed = list(predict_d.values())\n",
    "# Create a new dataset with ARIMA and XGBoost predictions\n",
    "X_combined = np.column_stack([best2_forecast_trimmed,best1_forecast_trimmed])\n",
    "# Normalize input features (optional but recommended)\n",
    "X_combined_scaled = scaler_minmax.fit_transform(X_combined)\n",
    "\n",
    "# Final predictions from the hybrid model\n",
    "hybrid_predictions_stacked = meta_model.predict(X_combined_scaled)\n",
    "\n",
    "# Create a new dataset with ARIMA and XGBoost predictions\n",
    "X_combined = np.column_stack(list(predict_d.values()))\n",
    "# Normalize input features (optional but recommended)\n",
    "X_combined_scaled = scaler_minmax.fit_transform(X_combined)\n",
    "\n",
    "# Final predictions from the hybrid model\n",
    "hybrid_predictions_stacked = meta_model.predict(X_combined_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c8145-ff5c-4e40-8b88-cec070a45a42",
   "metadata": {},
   "source": [
    "### 8.3 Predict Price with Stacked Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "950818b5-f757-4e30-9efc-6f8e1f340441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted future values from the hybrid model:\n",
      "            Hybrid Forecast\n",
      "2025-04-26        68.054378\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "n = 1 # Number of future predictions\n",
    "\n",
    "# Step 1: Ensure Predictions Are in NumPy Format\n",
    "future_best1_values = np.array(best1_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "future_best2_values = np.array(best2_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "\n",
    "# Step 2: Prepare Feature Matrix for Future Predictions\n",
    "X_combined_future = np.column_stack([future_best1_values, future_best2_values])\n",
    "\n",
    "# Step 3: Fit MinMaxScaler on Training Data (if not already fitted)\n",
    "scaler_minmax.fit(X_combined)  # Ensure the scaler is trained on past data\n",
    "X_combined_future_scaled = scaler_minmax.transform(X_combined_future)\n",
    "\n",
    "# Step 4: Predict Future Prices Using Hybrid Model\n",
    "hybrid_predictions_future = meta_model.predict(X_combined_future_scaled)\n",
    "\n",
    "# Step 5: Convert to 1D array\n",
    "hybrid_predictions_future = hybrid_predictions_future.flatten()\n",
    "\n",
    "# Step 6: Store Predictions in DataFrame\n",
    "future_dates = pd.date_range(start=y_test.index[-1], periods=n + 1, freq='D')[1:]\n",
    "future_price = pd.DataFrame({\n",
    "    'Hybrid Forecast': hybrid_predictions_future\n",
    "}, index=future_dates)\n",
    "\n",
    "# Step 7: Display Predicted Future Prices\n",
    "print(\"Predicted future values from the hybrid model:\")\n",
    "print(future_price)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ffc83d-cfd7-447f-87cc-129d63a79d7c",
   "metadata": {},
   "source": [
    "### 8.4 Let's verify Price with Weighted Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9ba22c1f-8369-4e45-8f28-e8f4ca110018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "min_length = min(len(best1_forecast_trimmed),len(best2_forecast_trimmed))\n",
    "y_test_trimmed = y_test[:min_length].values.flatten()\n",
    "\n",
    "# Calculate RMSE for both models\n",
    "rmse_best1 = sqrt(mean_squared_error(y_test_trimmed, best1_forecast_trimmed))\n",
    "rmse_best2 = sqrt(mean_squared_error(y_test_trimmed, best2_forecast_trimmed))\n",
    "\n",
    "# Compute weights using inverse RMSE\n",
    "weight_best1 = (1 / rmse_best1) / ((1 / rmse_best1) + (1 / rmse_best2))\n",
    "weight_best2 = (1 / rmse_best2) / ((1 / rmse_best1) + (1 / rmse_best2))\n",
    "\n",
    "# Weighted hybrid model predictions\n",
    "hybrid_weighted_predictions = (weight_best1 * best1_forecast_trimmed) + (weight_best2 * best2_forecast_trimmed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68402ad-ebf6-4330-bb48-a6906893604c",
   "metadata": {},
   "source": [
    "### 8.5 Predict Price with Weighted Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c256007-7f61-4263-b511-5c0bcd6a1576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Hybrid_Weighted\n",
      "2025-04-26        62.421563\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Ensure Predictions Are in NumPy Format\n",
    "future_best1_values = np.array(best1_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "future_best2_values = np.array(best2_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "\n",
    "# Step 2: Compute Hybrid Model Predictions\n",
    "\n",
    "## 1. Simple Averaging Hybrid Model\n",
    "hybrid_avg_future = (future_best1_values.flatten() + future_best2_values.flatten()) / 2\n",
    "\n",
    "## 2. Weighted Hybrid Model (Based on RMSE Inverse Weights)\n",
    "weight_best1 = (1 / rmse_best1) / ((1 / rmse_best1) + (1 / rmse_best2))\n",
    "weight_best2 = (1 / rmse_best2) / ((1 / rmse_best1) + (1 / rmse_best2))\n",
    "hybrid_weighted_future = (weight_best1 * future_best1_values.flatten()) + (weight_best2 * future_best2_values.flatten())\n",
    "\n",
    "# Step 3: Store Predictions in DataFrame\n",
    "future_dates = pd.date_range(start=y_test.index[-1], periods=n + 1, freq='D')[1:]\n",
    "future_price = pd.DataFrame({\n",
    "    'Hybrid_Weighted': hybrid_weighted_future,\n",
    "}, index=future_dates)\n",
    "print(future_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2837b8-f173-4e52-9aa3-0acde10b9b8c",
   "metadata": {},
   "source": [
    "### 8.6 Future Price Prediction and recommendation to Buy or Sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "80cdec1c-19da-4bb0-bcaa-b06ef65a6bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Actual Observed Price: 63.02\n",
      "\n",
      "Predicted future values and investment recommendation:\n",
      "            Future Price_weighted  Future Price_stacked  Future Price_average  \\\n",
      "2025-04-26              62.421563             68.054378             62.375747   \n",
      "\n",
      "           Recommendation  \n",
      "2025-04-26        HOLD ⚖️  \n",
      "\n",
      " Evaluation Metrics (on last known actual prices):\n",
      "             Model  R2 Score    MAPE    RMSE     MAE\n",
      "0  Weighted Hybrid       NaN  0.9496  0.5984  0.5984\n",
      "1   Stacked Hybrid       NaN  7.9885  5.0344  5.0344\n",
      "2   Average Hybrid       NaN  1.0223  0.6443  0.6443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n",
      "C:\\Users\\Lenovo\\myvenv\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:918: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
      "  warnings.warn(msg, UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "n = 1  # Number of future predictions\n",
    "threshold = 100  # Decision threshold in USD\n",
    "\n",
    "# Step 1: Ensure Predictions Are in NumPy Format\n",
    "future_best1_values = np.array(best1_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "future_best2_values = np.array(best2_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "\n",
    "# Step 2: Prepare Feature Matrix for Future Predictions\n",
    "X_combined_future = np.column_stack([future_best1_values, future_best2_values])\n",
    "\n",
    "# Step 3: Scale and Predict\n",
    "scaler_minmax.fit(X_combined)  # Use previously prepared training combined features\n",
    "X_combined_future_scaled = scaler_minmax.transform(X_combined_future)\n",
    "\n",
    "# Hybrid predictions\n",
    "hybrid_predictions_stacked = meta_model.predict(X_combined_future_scaled).flatten()\n",
    "hybrid_predictions_weighted = (weight_best1 * future_best1_values.flatten()) + (weight_best2 * future_best2_values.flatten())\n",
    "hybrid_predictions_average = (future_best1_values.flatten() + future_best2_values.flatten()) / 2\n",
    "\n",
    "# Step 4: Store Future Predictions\n",
    "future_dates = pd.date_range(start=y_test.index[-1], periods=n + 1, freq='D')[1:]\n",
    "future_price = pd.DataFrame({\n",
    "    'Future Price_weighted': hybrid_predictions_weighted,\n",
    "    'Future Price_stacked': hybrid_predictions_stacked,\n",
    "    'Future Price_average': hybrid_predictions_average\n",
    "}, index=future_dates)\n",
    "\n",
    "# Step 5: Last Observed Price\n",
    "last_actual_price = y_test.values[-1]\n",
    "\n",
    "# Step 6: Recommendation Based on Stacked Hybrid\n",
    "recommendation = []\n",
    "for forecast in hybrid_predictions_stacked:\n",
    "    price_diff = forecast - last_actual_price\n",
    "    if price_diff > threshold:\n",
    "        recommendation.append(\"BUY 📈\")\n",
    "    elif price_diff < -threshold:\n",
    "        recommendation.append(\"SELL 📉\")\n",
    "    else:\n",
    "        recommendation.append(\"HOLD ⚖️\")\n",
    "\n",
    "future_price['Recommendation'] = recommendation\n",
    "\n",
    "# Display Predictions\n",
    "print(f\"Last Actual Observed Price: {last_actual_price:.2f}\\n\")\n",
    "print(\"Predicted future values and investment recommendation:\")\n",
    "print(future_price)\n",
    "\n",
    "# ======================================\n",
    "# Evaluation Metrics on Known y_test[-n:]\n",
    "# ======================================\n",
    "actual_values = y_test[-n:].values.flatten()\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Store results\n",
    "metrics_summary = {\n",
    "    \"Model\": [\"Weighted Hybrid\", \"Stacked Hybrid\", \"Average Hybrid\"],\n",
    "    \"R2 Score\": [\n",
    "        r2_score(actual_values, hybrid_predictions_weighted),\n",
    "        r2_score(actual_values, hybrid_predictions_stacked),\n",
    "        r2_score(actual_values, hybrid_predictions_average),\n",
    "    ],\n",
    "    \"MAPE\": [\n",
    "        mape(actual_values, hybrid_predictions_weighted),\n",
    "        mape(actual_values, hybrid_predictions_stacked),\n",
    "        mape(actual_values, hybrid_predictions_average),\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        np.sqrt(mean_squared_error(actual_values, hybrid_predictions_weighted)),\n",
    "        np.sqrt(mean_squared_error(actual_values, hybrid_predictions_stacked)),\n",
    "        np.sqrt(mean_squared_error(actual_values, hybrid_predictions_average)),\n",
    "    ],\n",
    "    \"MAE\": [\n",
    "        mean_absolute_error(actual_values, hybrid_predictions_weighted),\n",
    "        mean_absolute_error(actual_values, hybrid_predictions_stacked),\n",
    "        mean_absolute_error(actual_values, hybrid_predictions_average),\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and show\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(\"\\n Evaluation Metrics (on last known actual prices):\")\n",
    "print(metrics_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "df537ec3-1dc0-461b-89bd-e118ff307688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluation Metrics (on last known actual prices) with historical data from 2003:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>R2 Score</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Weighted Hybrid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.94960</td>\n",
       "      <td>0.598438</td>\n",
       "      <td>0.598438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacked Hybrid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.98854</td>\n",
       "      <td>5.034378</td>\n",
       "      <td>5.034378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Average Hybrid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.02230</td>\n",
       "      <td>0.644253</td>\n",
       "      <td>0.644253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  R2 Score     MAPE      RMSE       MAE\n",
       "0  Weighted Hybrid       NaN  0.94960  0.598438  0.598438\n",
       "1   Stacked Hybrid       NaN  7.98854  5.034378  5.034378\n",
       "2   Average Hybrid       NaN  1.02230  0.644253  0.644253"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\n Evaluation Metrics (on last known actual prices) with historical data from 2003:\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88c49695-41d5-4bb6-9e5a-7627dd43b0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Actual Observed Price: 63.02\n",
      "\n",
      "Predicted future values and investment recommendation:\n",
      "            Future Price Weighted  Future Price Stacked  Future Price Average  \\\n",
      "2025-04-26              62.421563             68.054378             62.375747   \n",
      "2025-04-27              62.421563             68.054378             62.375747   \n",
      "2025-04-28              62.421563             68.054378             62.375747   \n",
      "2025-04-29              62.421563             68.054378             62.375747   \n",
      "2025-04-30              62.421563             68.054378             62.375747   \n",
      "\n",
      "           Recommendation  \n",
      "2025-04-26        HOLD ⚖️  \n",
      "2025-04-27        HOLD ⚖️  \n",
      "2025-04-28        HOLD ⚖️  \n",
      "2025-04-29        HOLD ⚖️  \n",
      "2025-04-30        HOLD ⚖️  \n",
      "\n",
      "Evaluation Metrics (on last known actual prices):\n",
      "             Model    MAPE    RMSE     MAE  Accuracy (%)\n",
      "0   Average Hybrid  1.0223  0.6443  0.6443       98.9777\n",
      "1  Weighted Hybrid  0.9496  0.5984  0.5984       99.0504\n",
      "2   Stacked Hybrid  7.9885  5.0344  5.0344       92.0115\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "n = 5  # Number of future predictions\n",
    "threshold = 5  # Decision threshold in USD\n",
    "\n",
    "# Step 1: Ensure Predictions Are in NumPy Format\n",
    "future_best1_values = np.array(best1_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "future_best2_values = np.array(best2_forecast_trimmed[-n:]).reshape(-1, 1)\n",
    "\n",
    "# Step 2: Prepare Feature Matrix for Future Predictions\n",
    "X_combined_future = np.column_stack([future_best1_values, future_best2_values])\n",
    "\n",
    "# Step 3: Scale and Predict\n",
    "scaler_minmax.fit(X_combined)  # Use previously prepared training combined features\n",
    "X_combined_future_scaled = scaler_minmax.transform(X_combined_future)\n",
    "\n",
    "# Hybrid predictions\n",
    "hybrid_predictions_weighted = (weight_best1 * future_best1_values.flatten()) + (weight_best2 * future_best2_values.flatten())\n",
    "hybrid_predictions_stacked = meta_model.predict(X_combined_future_scaled).flatten()\n",
    "hybrid_predictions_average = (future_best1_values.flatten() + future_best2_values.flatten()) / 2\n",
    "\n",
    "# Step 4: Store Future Predictions\n",
    "future_dates = pd.date_range(start=y_test.index[-1], periods=n + 1, freq='D')[1:]\n",
    "future_price = pd.DataFrame({\n",
    "    'Future Price Weighted': hybrid_predictions_weighted,\n",
    "    'Future Price Stacked': hybrid_predictions_stacked,\n",
    "    'Future Price Average': hybrid_predictions_average\n",
    "}, index=future_dates)\n",
    "\n",
    "# Step 5: Last Observed Price\n",
    "last_actual_price = y_test.values[-1]\n",
    "\n",
    "# Step 6: Recommendation Based on Weighted Hybrid\n",
    "recommendation = []\n",
    "for forecast in hybrid_predictions_weighted:\n",
    "    price_diff = forecast - last_actual_price\n",
    "    if price_diff > threshold:\n",
    "        recommendation.append(\"BUY 📈\")\n",
    "    elif price_diff < -threshold:\n",
    "        recommendation.append(\"SELL 📉\")\n",
    "    else:\n",
    "        recommendation.append(\"HOLD ⚖️\")\n",
    "\n",
    "future_price['Recommendation'] = recommendation\n",
    "\n",
    "# Display Predictions\n",
    "print(f\"Last Actual Observed Price: {last_actual_price:.2f}\\n\")\n",
    "print(\"Predicted future values and investment recommendation:\")\n",
    "print(future_price)\n",
    "\n",
    "# ======================================\n",
    "# Evaluation Metrics on Known y_test[-n:]\n",
    "# ======================================\n",
    "actual_values = y_test[-n:].values.flatten()\n",
    "\n",
    "def mape(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return 100 - mape(y_true, y_pred)\n",
    "\n",
    "# Store results\n",
    "metrics_summary = {\n",
    "    \"Model\": [\"Average Hybrid\", \"Weighted Hybrid\", \"Stacked Hybrid\"],\n",
    "    \n",
    "    \"MAPE\": [\n",
    "        mape(actual_values, hybrid_predictions_average),\n",
    "        mape(actual_values, hybrid_predictions_weighted),\n",
    "        mape(actual_values, hybrid_predictions_stacked)\n",
    "    ],\n",
    "    \"RMSE\": [\n",
    "        np.sqrt(mean_squared_error(actual_values, hybrid_predictions_average)),\n",
    "        np.sqrt(mean_squared_error(actual_values, hybrid_predictions_weighted)),\n",
    "        np.sqrt(mean_squared_error(actual_values, hybrid_predictions_stacked))\n",
    "    ],\n",
    "    \"MAE\": [\n",
    "        mean_absolute_error(actual_values, hybrid_predictions_average),\n",
    "        mean_absolute_error(actual_values, hybrid_predictions_weighted),\n",
    "        mean_absolute_error(actual_values, hybrid_predictions_stacked)\n",
    "    ],\n",
    "    \"Accuracy (%)\": [\n",
    "        accuracy(actual_values, hybrid_predictions_average),\n",
    "        accuracy(actual_values, hybrid_predictions_weighted),\n",
    "        accuracy(actual_values, hybrid_predictions_stacked)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame and show\n",
    "metrics_df = pd.DataFrame(metrics_summary)\n",
    "print(\"\\nEvaluation Metrics (on last known actual prices):\")\n",
    "print(metrics_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2098a754-5d68-4670-84f4-8d0685d6a865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Accuracy (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average Hybrid</td>\n",
       "      <td>1.0223</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>98.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weighted Hybrid</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>0.5984</td>\n",
       "      <td>99.0504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stacked Hybrid</td>\n",
       "      <td>7.9885</td>\n",
       "      <td>5.0344</td>\n",
       "      <td>5.0344</td>\n",
       "      <td>92.0115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model    MAPE    RMSE     MAE  Accuracy (%)\n",
       "0   Average Hybrid  1.0223  0.6443  0.6443       98.9777\n",
       "1  Weighted Hybrid  0.9496  0.5984  0.5984       99.0504\n",
       "2   Stacked Hybrid  7.9885  5.0344  5.0344       92.0115"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63213624-5aa3-45d0-93db-aecdf70691ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7aef6-ee7c-40b1-b330-2f658c8600fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468d8a36-4255-4533-bd32-ccec992dcede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845e7e2-6ea8-4286-be21-1d5270c73298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625b6d6-06f4-44b3-91de-dbb17454326c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fdf96f-cc4e-484a-a0c7-e690c756e73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a74e8b-64ee-4111-a72c-682c512ea2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcaea2-7a41-4c72-b636-7b415273088f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5da76fb-abf5-47df-978b-f228359ee343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832f94e-dc11-40b7-b1fd-cf06e9d8649a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fdd46-3344-4078-bd60-15a1fb266464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd540f6-3ecb-4114-9cc7-5a32f416ab42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb263913-bf96-4118-9fb8-d615cb0535b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32019d8-d4db-46cf-b530-c825698689b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee20da-8875-4434-8f32-f40c2a89c63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d8275-451f-4b40-8e0a-a6f7a9c26801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce16445-fcba-4727-a874-57a43a267fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e498463-34d0-475b-8ee5-9d2690ca9ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc859a9-e348-4431-bba5-df3d3aee12ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ea4da-baa6-496a-a3bf-166812b56406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f714a-223a-4e9e-b318-0f838f1d03f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
